{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FinalPipeline.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP58MCvFUxw75Q9+8wPXQnQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"shgko9FyxuyM"},"source":["#!pip install tensorflow-gpu==2.3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2YKzFaHmx-H-","executionInfo":{"status":"ok","timestamp":1617945488089,"user_tz":-330,"elapsed":3718,"user":{"displayName":"NIDHI AGRAWAL","photoUrl":"","userId":"09931979865334263524"}}},"source":["from google.colab import files\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.layers import Dense,Input,GRU,Embedding,Flatten\n","from tensorflow.keras.models import Model\n","import keras.backend as K\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","import seaborn as sns\n","import matplotlib as plt\n","from joblib import dump, load"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"VycsW0kzyEuE","executionInfo":{"status":"ok","timestamp":1617945511137,"user_tz":-330,"elapsed":1147,"user":{"displayName":"NIDHI AGRAWAL","photoUrl":"","userId":"09931979865334263524"}}},"source":["class Encoder(tf.keras.Model):\n","    '''\n","    Encoder model -- That takes a input sequence and returns output sequence\n","    '''\n"," \n","    def __init__(self,inp_vocab_size,embedding_size,lstm_size,input_length):\n","        self.inp_vocab_size=inp_vocab_size\n","        self.embedding_size=embedding_size\n","        self.lstm_size=lstm_size\n","        self.input_length=input_length\n"," \n","        #Initialize Embedding layer\n"," \n","        #Intialize Encoder LSTM layer\n","        super().__init__()\n","        self.Embedding_Layer=tf.keras.layers.Embedding(input_dim=self.inp_vocab_size,output_dim=self.embedding_size,input_length=input_length,mask_zero=True,name=\"Encoder_Embedding\")\n","        self.LSTM_Layer=tf.keras.layers.LSTM(self.lstm_size,return_state=True,return_sequences=True,name=\"Encoder_LSTM\")\n"," \n","    def call(self,input_sequence,states):\n","      '''\n","          This function takes a sequence input and the initial states of the encoder.\n","          Pass the input_sequence input to the Embedding layer, Pass the embedding layer ouput to encoder_lstm\n","          returns -- All encoder_outputs, last time steps hidden and cell state\n","      '''\n","      input_embedd=self.Embedding_Layer(input_sequence)\n","      self.lstm_output, self.lstm_state_h,self.lstm_state_c = self.LSTM_Layer(input_embedd)\n","     \n","      return self.lstm_output,self.lstm_state_h,self.lstm_state_c\n"," \n","    \n","    def initialize_states(self,batch_size):\n","      '''\n","      Given a batch size it will return intial hidden state and intial cell state.\n","      If batch size is 32- Hidden state is zeros of size [32,lstm_units], cell state zeros is of size [32,lstm_units]\n","      '''\n","      intial_hidden_state=np.zeros((batch_size,self.lstm_size))\n","      intial_cell_state=np.zeros((batch_size,self.lstm_size))\n","      return intial_hidden_state,intial_cell_state"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"zy1ar4ioyIa5","executionInfo":{"status":"ok","timestamp":1617945519094,"user_tz":-330,"elapsed":1252,"user":{"displayName":"NIDHI AGRAWAL","photoUrl":"","userId":"09931979865334263524"}}},"source":["class Attention(tf.keras.layers.Layer):\n","  '''\n","    Class the calculates score based on the scoring_function using Bahdanu attention mechanism.\n","  '''\n","  def __init__(self,scoring_function, att_units):\n","    self.scoring_function=scoring_function\n","    self.att_units=att_units \n","    super().__init__()\n"," \n","    # Please go through the reference notebook and research paper to complete the scoring functions\n"," \n","    if self.scoring_function=='dot':\n","      # Intialize variables needed for Dot score function here\n","      pass\n"," \n","      \n","      \n","    if scoring_function == 'general':\n","      # Intialize variables needed for General score function here\n","      self.W=tf.keras.layers.Dense(att_units)\n","    elif scoring_function == 'concat':\n","      # Intialize variables needed for Concat score function here\n","      self.W1 = tf.keras.layers.Dense(att_units)\n","      self.W2 = tf.keras.layers.Dense(att_units)\n","      self.V = tf.keras.layers.Dense(1)\n","  \n","  \n","  def call(self,decoder_hidden_state,encoder_output):\n","    '''\n","      Attention mechanism takes two inputs current step -- decoder_hidden_state and all the encoder_outputs.\n","      * Based on the scoring function we will find the score or similarity between decoder_hidden_state and encoder_output.\n","        Multiply the score function with your encoder_outputs to get the context vector.\n","        Function returns context vector and attention weights(softmax - scores)\n","    '''\n","    \n","    if self.scoring_function == 'dot':\n","        # Implement Dot score function here\n","        \n","        \n","        scoring=tf.matmul(encoder_output,tf.expand_dims(decoder_hidden_state,axis=-1))\n","        \n","        attention_weight=tf.nn.softmax(scoring,axis=1)\n","        \n","        context_vector=attention_weight*encoder_output\n","        context_vector=tf.reduce_sum(context_vector,axis=1)\n","        return context_vector,attention_weight\n","              \n","        \n","        \n","        \n"," \n","        \n","    elif self.scoring_function == 'general':\n","        # Implement General score function here\n","        \n","        score=tf.matmul(self.W(encoder_output),tf.expand_dims(decoder_hidden_state,axis=-1))\n","        attention_weight=tf.nn.softmax(score,axis=1)\n","        context_vector=attention_weight*encoder_output\n","        context_vector=tf.reduce_sum(context_vector,axis=1)\n","        return context_vector,attention_weight\n","    \n","    elif self.scoring_function == 'concat':\n","        # Implement General score function here\n","        score = self.V(tf.nn.tanh(self.W1(encoder_output) + self.W2(tf.expand_dims(decoder_hidden_state,axis=1))))\n","        attention_weight=tf.nn.softmax(score,axis=1)\n","        context_vector=attention_weight*encoder_output\n","        context_vector=tf.reduce_sum(context_vector,axis=1)\n","        return context_vector,attention_weight"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vg3bLOpJyMPl","executionInfo":{"status":"ok","timestamp":1617945524940,"user_tz":-330,"elapsed":988,"user":{"displayName":"NIDHI AGRAWAL","photoUrl":"","userId":"09931979865334263524"}}},"source":["class OneStepDecoder(tf.keras.Model):\n","  def __init__(self,tar_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n","\n","      # Initialize decoder embedding layer, LSTM and any other objects needed\n","      self.tar_vocab_size=tar_vocab_size\n","      self.embedding_dim=embedding_dim\n","      self.input_length=input_length\n","      self.dec_units=dec_units\n","      self.score_fun=score_fun\n","      self.att_units=att_units\n","      super().__init__()\n","      self.DEmbedding=tf.keras.layers.Embedding(input_dim=self.tar_vocab_size,output_dim=self.embedding_dim,input_length=self.input_length,mask_zero=True,name=\"Decoder_Embedding\", trainable=False)\n","      self.DLSTM_Layer=tf.keras.layers.LSTM(self.dec_units,return_state=True,return_sequences=True,name=\"Encoder_LSTM\")\n","      self.full_connected=tf.keras.layers.Dense(self.tar_vocab_size)\n","      self.attention=Attention(self.score_fun,self.att_units)\n","\n","\n","\n","\n","  def call(self,input_to_decoder, encoder_output, state_h,state_c):\n","    '''\n","        One step decoder mechanisim step by step:\n","      A. Pass the input_to_decoder to the embedding layer and then get the output(batch_size,1,embedding_dim)\n","      B. Using the encoder_output and decoder hidden state, compute the context vector.\n","      C. Concat the context vector with the step A output\n","      D. Pass the Step-C output to LSTM/GRU and get the decoder output and states(hidden and cell state)\n","      E. Pass the decoder output to dense layer(vocab size) and store the result into output.\n","      F. Return the states from step D, output from Step E, attention weights from Step -B\n","    '''\n","    #step-A\n","    output=self.DEmbedding(input_to_decoder)\n","    #step-B\n","   \n","    context_vector,attention_weights=self.attention(state_h,encoder_output)\n","    #step-C\n","  \n","    concat=tf.concat([tf.expand_dims(context_vector,1),output],axis=-1)\n","    #step-D\n","    decoder_output,hidden_states,cell_states=self.DLSTM_Layer(concat,initial_state=[state_h,state_c])\n","    decoder_output=tf.reshape(decoder_output,(-1,decoder_output.shape[2]))\n","    output=self.full_connected(decoder_output)\n","    return output,hidden_states,cell_states,attention_weights,context_vector"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"agV0z07oyPZP","executionInfo":{"status":"ok","timestamp":1617945529772,"user_tz":-330,"elapsed":999,"user":{"displayName":"NIDHI AGRAWAL","photoUrl":"","userId":"09931979865334263524"}}},"source":["class Decoder(tf.keras.Model):\n","    def __init__(self,out_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n","      #Intialize necessary variables and create an object from the class onestepdecoder\n","      self.out_vocab_size=out_vocab_size\n","      self.embedding_dim=embedding_dim\n","      self.input_length=input_length\n","      self.dec_units=dec_units\n","      self.score_fun=score_fun\n","      self.att_units=att_units\n","      super().__init__()\n","      self.onestepdecoder=OneStepDecoder(self.out_vocab_size, self.embedding_dim, self.input_length, self.dec_units ,self.score_fun ,self.att_units)\n","\n","\n","     \n","    def call(self, input_to_decoder,encoder_output,decoder_hidden_state,decoder_cell_state ):\n","\n","        #Initialize an empty Tensor array, that will store the outputs at each and every time step\n","        #Create a tensor array as shown in the reference notebook\n","        \n","        #Iterate till the length of the decoder input\n","            # Call onestepdecoder for each token in decoder_input\n","            # Store the output in tensorarray\n","        # Return the tensor array\n","        \n","        all_outputs=tf.TensorArray(tf.float32,size=tf.shape(input_to_decoder)[1],name=\"output_arrays\")\n","      \n","        \n","        for timestep in range(tf.shape(input_to_decoder)[1]):\n","            output,decoder_hidden_state,decoder_cell_state,_,_=self.onestepdecoder(input_to_decoder[:,timestep:timestep+1],encoder_output,decoder_hidden_state,decoder_cell_state)\n","            all_outputs=all_outputs.write(timestep,output)\n","            \n","        all_outputs=tf.transpose(all_outputs.stack(),[1,0,2])\n","        return all_outputs\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"B0qGrWpcyTT0","executionInfo":{"status":"ok","timestamp":1617945533787,"user_tz":-330,"elapsed":990,"user":{"displayName":"NIDHI AGRAWAL","photoUrl":"","userId":"09931979865334263524"}}},"source":["\n","class encoder_decoder(tf.keras.Model):\n","    def __init__(self,embedding_size,lstm_size,input_length,decoder_input_length,dec_units ,score_fun ,att_units, batch_size):\n","        #Intialize objects from encoder decoder\n","        super().__init__()\n","        self.encoder=Encoder(104,embedding_size,lstm_size,input_length)\n","        self.decoder=Decoder(93,embedding_size, decoder_input_length, dec_units ,score_fun ,att_units)\n","        self.batch_size= batch_size\n","    def call(self,data):\n","        #Intialize encoder states, Pass the encoder_sequence to the embedding layer\n","        # Decoder initial states are encoder final states, Initialize it accordingly\n","        # Pass the decoder sequence,encoder_output,decoder states to Decoder\n","        # return the decoder output\n","        input_,target_sentences = data[0], data[1]\n","        initial_state= self.encoder.initialize_states(self.batch_size)\n","        encoder_output,state_h,state_c=self.encoder(input_,initial_state)\n","        output=self.decoder(target_sentences,encoder_output, state_h, state_c)\n","        return output"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"wPFnHb3KWJou","executionInfo":{"status":"ok","timestamp":1617945538390,"user_tz":-330,"elapsed":966,"user":{"displayName":"NIDHI AGRAWAL","photoUrl":"","userId":"09931979865334263524"}}},"source":["def custom_lossfunction(targets,logits):\n","\n","  # Custom loss function that will not consider the loss for padded zeros.\n","  # Refer https://www.tensorflow.org/tutorials/text/nmt_with_attention#define_the_optimizer_and_the_loss_function\n","   \n","   loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n","   mask = tf.math.logical_not(tf.math.equal(targets, 0))\n","   loss_ = loss_object(targets, logits)\n","\n","   mask = tf.cast(mask, dtype=loss_.dtype)\n","   loss_ *= mask\n","\n","   return tf.reduce_mean(loss_)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"enZsCeo0Y8Kh","executionInfo":{"status":"ok","timestamp":1617945541262,"user_tz":-330,"elapsed":1136,"user":{"displayName":"NIDHI AGRAWAL","photoUrl":"","userId":"09931979865334263524"}}},"source":["class Dataset:\n","    def __init__(self, data, tknizer_ita, tknizer_eng, max_len,max_len1):\n","        self.encoder_inps = data['SMS_TEXT'].values\n","        self.decoder_inps = data['ENGLISH_INPUT'].values\n","        self.decoder_outs = data['ENGLISH_OUTPUT'].values\n","        self.tknizer_eng = tknizer_eng\n","        self.tknizer_ita = tknizer_ita\n","        self.max_len = max_len\n","        self.max_len1=max_len1\n"," \n","    def __getitem__(self, i):\n","        self.encoder_seq = self.tknizer_ita.texts_to_sequences([self.encoder_inps[i]]) # need to pass list of values\n","        self.decoder_inp_seq = self.tknizer_eng.texts_to_sequences([self.decoder_inps[i]])\n","        self.decoder_out_seq = self.tknizer_eng.texts_to_sequences([self.decoder_outs[i]])\n"," \n","        self.encoder_seq = pad_sequences(self.encoder_seq, maxlen=self.max_len, dtype='int32', padding='post')\n","        self.decoder_inp_seq = pad_sequences(self.decoder_inp_seq, maxlen=self.max_len1, dtype='int32', padding='post')\n","        self.decoder_out_seq = pad_sequences(self.decoder_out_seq, maxlen=self.max_len1, dtype='int32', padding='post')\n","        return self.encoder_seq, self.decoder_inp_seq, self.decoder_out_seq\n"," \n","    def __len__(self): # your model.fit_gen requires this function\n","        return len(self.encoder_inps)\n"," \n","    \n","class Dataloder(tf.keras.utils.Sequence):    \n","    def __init__(self, dataset, batch_size=1):\n","        self.dataset = dataset\n","        self.batch_size = batch_size\n","        self.indexes = np.arange(len(self.dataset.encoder_inps))\n"," \n"," \n","    def __getitem__(self, i):\n","        start = i * self.batch_size\n","        stop = (i + 1) * self.batch_size\n","        data = []\n","        for j in range(start, stop):\n","            data.append(self.dataset[j])\n"," \n","        batch = [np.squeeze(np.stack(samples, axis=1), axis=0) for samples in zip(*data)]\n","        # we are creating data like ([italian, english_inp], english_out) these are already converted into seq\n","        return tuple([[batch[0],batch[1]],batch[2]])\n"," \n","    def __len__(self):  # your model.fit_gen requires this function\n","        return len(self.indexes) // self.batch_size\n"," \n","    def on_epoch_end(self):\n","        self.indexes = np.random.permutation(self.indexes)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"_w6rLJntyt_Y","executionInfo":{"status":"ok","timestamp":1617945546975,"user_tz":-330,"elapsed":994,"user":{"displayName":"NIDHI AGRAWAL","photoUrl":"","userId":"09931979865334263524"}}},"source":["def load_model():\n","  \"\"\"1. Load test_dataloader from joblib files\n","     2. run the model for 1 epoch to compile and load_weights \n","     3. return the model with loaded weights.\"\"\"\n","  test_dataloader=load('/content/test_dataloader.joblib')   \n","  model2  = encoder_decoder(300,100,170,202,100,'concat',100,64)\n","  optimizer = tf.keras.optimizers.Adam(0.01)\n","  model2.compile(optimizer=optimizer,loss=custom_lossfunction)\n","  model2.fit_generator(test_dataloader, steps_per_epoch=1, epochs=1, validation_data=test_dataloader, validation_steps=1)\n","  model2.load_weights('/content/attention_model_0.3930.h5')\n","  return model2"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"YoMoxJYSgFhF","executionInfo":{"status":"ok","timestamp":1617953906129,"user_tz":-330,"elapsed":1877,"user":{"displayName":"NIDHI AGRAWAL","photoUrl":"","userId":"09931979865334263524"}}},"source":["class ModelPrediction():\n","  def __init__(self):\n","    \"\"\"Loading the tokenizer\n","    calling the load_model which returns the model with trained model weights\"\"\"\n","    self.tokenizer=load('/content/tokenizer.joblib') \n","    self.tokenizer_e=load('/content/tokenizer_e.joblib') \n","    self.model2=load_model()\n","  def prediction(self,input_sentence):\n","    \"\"\"\n","    1.Type of input sentence is list which works for n number of sentences in the list.\n","    2.Tokenizing,Padding,convertinng the sentences into tensor array\n","    3.Intialising hidden states of shape length of list and units.\n","    4.passing the tensor array and hidden states to encoder returns encoder output,hidden state and cell state\n","    5.assigning decoder hidden states with state of encoder \n","    6. Intialising the start token which is '\\t' for length of list times and assign as decoder input\n","    7. Intialisng list with '' for length of list times and append into the list.\n","    8. for loop till max length:\n","       1. passing encoder output,decoder input and states to one step decoder returns prediction and states \n","       2. find the max value in rows and send to tokenizer to get the character\n","       3. Appending the character into the string of respective indexes.\n","    9. Slicing the string to end token '\\n'\n","    10. Append into the final list and returns the final list.    \n","\n","\n","     \n","     \"\"\"\n","    n=len(input_sentence)\n","    input_sequence=self.tokenizer.texts_to_sequences(input_sentence)\n","    inputs=pad_sequences(input_sequence,maxlen=170,padding='post')\n","    inputs=tf.convert_to_tensor(inputs)\n","    result=''\n","    result1=''\n","    units=100\n","    hidden=[tf.zeros((n,units))]\n","    encoder_output,hidden_state,cell_state=self.model2.layers[0](inputs,hidden)\n","   \n","\n","    dec_hidden=hidden_state\n","    dec_input=tf.expand_dims([self.tokenizer_e.word_index['\\t']],0)\n","    dec_input=tf.repeat(dec_input,n,axis=0).numpy()\n","    dec_input=dec_input.reshape((n,1))\n","    print(dec_input.shape)\n","    print(dec_input)   \n","    output_array=[]\n","    for index in range(n):\n","      output_array.append('')\n","\n","    for t in range(202):\n","        predictions,dec_hidden,cell_state,_,_=self.model2.layers[1].onestepdecoder(dec_input,encoder_output,dec_hidden,cell_state)\n","        \n","        predicted_id=tf.argmax(predictions,axis=1).numpy()\n","        \n","        predicted_id=np.array(predicted_id).reshape((n,1))     \n","        predictions=self.tokenizer_e.sequences_to_texts(predicted_id)\n","        for index in range(len(output_array)):\n","          output_array[index]+=predictions[index]\n","        dec_input=predicted_id\n","    \n","       \n","    final_output=[]\n","    for index in range(len(output_array)):\n","      string_value= output_array[index][:output_array[index].find('\\n')]  \n","      final_output.append(string_value)\n","        \n","    return final_output"],"execution_count":97,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"baV-tpD4ghHN","executionInfo":{"status":"ok","timestamp":1617953922769,"user_tz":-330,"elapsed":13407,"user":{"displayName":"NIDHI AGRAWAL","photoUrl":"","userId":"09931979865334263524"}},"outputId":"8888916e-8bbf-4aab-ca56-548ca90b18a0"},"source":["model=ModelPrediction()"],"execution_count":98,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f1b93a63b90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - ETA: 0s - loss: 1.8063WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f1c3e2f7b00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 10s 10s/step - loss: 1.8063 - val_loss: 1.7629\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lv2ndCIpJnDn","executionInfo":{"status":"ok","timestamp":1617953929473,"user_tz":-330,"elapsed":2735,"user":{"displayName":"NIDHI AGRAWAL","photoUrl":"","userId":"09931979865334263524"}},"outputId":"1003f7c8-1d73-434b-d8ed-786622c26f5c"},"source":["list1=[\"Hello\",\"Hi\",\"bye\",'Lets go']\n","model.prediction(list1)"],"execution_count":99,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Hh. elo.', 'Hi.', 'Cwy o ee.', 'Hl.e ets o o es.']"]},"metadata":{"tags":[]},"execution_count":99}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nPDdiOcikcfu","executionInfo":{"status":"ok","timestamp":1617953978397,"user_tz":-330,"elapsed":2216,"user":{"displayName":"NIDHI AGRAWAL","photoUrl":"","userId":"09931979865334263524"}},"outputId":"8e5a201b-4ce8-43af-c6bd-48bfa5769b08"},"source":["list2=[\"Lets go to New York\",\"Uttar Pradesh\"]\n","model.prediction(list2)"],"execution_count":100,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Hele ets g o nt ew o?', 'A ottr ar t adprae.']"]},"metadata":{"tags":[]},"execution_count":100}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"arCAgGNzkpqe","executionInfo":{"status":"ok","timestamp":1617954008702,"user_tz":-330,"elapsed":2416,"user":{"displayName":"NIDHI AGRAWAL","photoUrl":"","userId":"09931979865334263524"}},"outputId":"395fd6ec-2703-41a4-b855-46ab9e903f9e"},"source":["list3=[\"Lets\"]\n","model.prediction(list3)"],"execution_count":102,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Wleet ots.']"]},"metadata":{"tags":[]},"execution_count":102}]}]}