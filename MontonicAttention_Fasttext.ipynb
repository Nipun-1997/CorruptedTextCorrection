{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MontonicAttention_Fasttext.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOSWJvmvoUo99iREigYYmrC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"LVX1bxoaYHwo"},"source":["#!pip install tensorflow-gpu==2.3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tfWydMwnY1ml","executionInfo":{"status":"ok","timestamp":1617164529520,"user_tz":-330,"elapsed":2547,"user":{"displayName":"Nipun Agrawal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEnTNOPKYA066Ly1RwsNs4132AgrMH5fVw2BuEAw=s64","userId":"18285146514188802906"}}},"source":["import tensorflow as tf\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import Layer\n","from tensorflow.keras import initializers, regularizers, constraints\n","import six\n","from tensorflow.keras.utils import deserialize_keras_object\n","from joblib import dump, load"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"SHQc-grYYsM7","executionInfo":{"status":"ok","timestamp":1617164548198,"user_tz":-330,"elapsed":940,"user":{"displayName":"Nipun Agrawal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEnTNOPKYA066Ly1RwsNs4132AgrMH5fVw2BuEAw=s64","userId":"18285146514188802906"}}},"source":["from google.colab import files\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.layers import Dense,Input,GRU,Embedding,Flatten\n","from tensorflow.keras.models import Model\n","import keras.backend as K\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from tensorflow.keras.layers import Input, Dense, Conv2D, BatchNormalization, Activation, Dropout, GRU, Embedding\n","from tensorflow.keras.models import Model\n","from tensorflow.keras import activations\n","from tensorflow.keras.layers import Layer\n","from tensorflow.keras import layers\n","import tensorflow as tf\n"," \n","from tensorflow.keras.layers import concatenate, Lambda"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"AV4EcHXfYvER","executionInfo":{"status":"ok","timestamp":1617164557980,"user_tz":-330,"elapsed":6454,"user":{"displayName":"Nipun Agrawal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEnTNOPKYA066Ly1RwsNs4132AgrMH5fVw2BuEAw=s64","userId":"18285146514188802906"}},"outputId":"d8b7f63c-df44-4853-97ec-39e4e06c643a"},"source":["tf.test.gpu_device_name()"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/device:GPU:0'"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"Znp9Yy2yZ3kr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617164640890,"user_tz":-330,"elapsed":1140,"user":{"displayName":"Nipun Agrawal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEnTNOPKYA066Ly1RwsNs4132AgrMH5fVw2BuEAw=s64","userId":"18285146514188802906"}},"outputId":"31868491-d1eb-43a5-d2b7-cf16b6572e27"},"source":["\n","ENCODER_SEQ_LEN = 38\n","DECODER_SEQ_LEN = 39\n","EN_VOCAB_SIZE = len(load('/content/encoder_vocab_lower.joblib'))+1\n","DE_VOCAB_SIZE=len(load('/content/decoder_vocab_lower.joblib'))+1\n","units = 100\n","x = load('/content/encoder_train.joblib')\n","y = load('/content/decoder_inp_train.joblib')\n","z = load('/content/decoder_out_train.joblib')\n","encoder_test=load('/content/encoder_test.joblib')\n","decoder_inp_test=load('/content/decoder_inp_test.joblib')\n","decoder_out_test=load('/content/decoder_out_test.joblib')\n","decoder_embedding=load('/content/decoder_embedding2.joblib')\n","print(decoder_embedding.shape)\n","encoder_embedding=load('/content/encoder_embedding1.joblib')\n","print(encoder_embedding.shape)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["(3040, 300)\n","(3702, 300)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mRod_4SLZFCb","executionInfo":{"status":"ok","timestamp":1617164646673,"user_tz":-330,"elapsed":1341,"user":{"displayName":"Nipun Agrawal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEnTNOPKYA066Ly1RwsNs4132AgrMH5fVw2BuEAw=s64","userId":"18285146514188802906"}}},"source":["def serialize(activation):\n","    '''Get the name of activation'''\n","    return activation.__name__\n","def get(identifier):\n","    \"\"\"Get the `identifier` activation function.\n","    # Arguments\n","        identifier: None or str, name of the function.\n","    # Returns\n","        The activation function, `linear` if `identifier` is None.\n","    # Raises\n","        ValueError if unknown identifier\n","    \"\"\"\n","    if isinstance(identifier, six.string_types):\n","        identifier = str(identifier)\n","        return deserialize(identifier)\n","    elif callable(identifier):\n","        if isinstance(identifier, Layer):\n","            warnings.warn(\n","                'Do not pass a layer instance (such as {identifier}) as the '\n","                'activation argument of another layer. Instead, advanced '\n","                'activation layers should be used just like any other '\n","                'layer in a model.'.format(\n","                    identifier=identifier.__class__.__name__))\n","        return identifier\n","    else:\n","        raise ValueError('Could not interpret '\n","                         'activation function identifier:', identifier) \n","        \n","def _monotonic_attetion(probabilities, attention_prev, mode):\n","\n","    \"\"\"Compute monotonic attention distribution from choosing probabilities.\n","    Implemented Based on -\n","    https://colinraffel.com/blog/online-and-linear-time-attention-by-enforcing-monotonic-alignments.html\n","    https://arxiv.org/pdf/1704.00784.pdf\n","    Mainly implemented by referring\n","    https://github.com/craffel/mad/blob/b3687a70615044359c8acc440e43a5e23dc58309/example_decoder.py#L22\n","    # Arguments:\n","        probabilities: Probability of choosing input sequence..\n","                       Should be of shape (batch_size, max_length),\n","                       and should all be in the range [0, 1].\n","        attention_prev: The attention distribution from the previous output timestep.\n","                            Should be of shape (batch_size, max_length).\n","                            For the first output timestep,\n","                            should be [1, 0, 0, ...,0] for all n in [0, ... batch_size - 1].\n","        mode: How to compute the attention distribution.\n","              Must be one of 'recursive', 'parallel', or 'hard'.\n","              - 'recursive' uses tf.scan to recursively compute the distribution.\n","              This is slowest but is exact, general, and does not suffer from\n","              numerical instabilities.\n","              - 'parallel' uses parallelized cumulative-sum and cumulative-product\n","              operations to compute a closed-form solution to the recurrence relation\n","              defining the attention distribution.  This makes it more efficient than 'recursive',\n","              but it requires numerical checks which make the distribution non-exact.\n","              This can be a problem in particular when max_length is long and/or\n","              probabilities has entries very close to 0 or 1.\n","              - 'hard' requires that  the probabilities in p_choose_i are all either 0 or 1,\n","              and subsequently uses a more efficient and exact solution.\n","    # Returns: A tensor of shape (batch_size, max_length) representing the attention distributions\n","               for each sequence in the batch.\n","    # Raises:\n","             ValueError: if mode is not one of 'recursive', 'parallel', 'hard'.\"\"\"\n","    if mode == 'hard':\n","        #Remove any probabilities before the index chosen last time step\n","        probabilities = probabilities*tf.cumsum(attention_prev, axis=1)\n","        attention = probabilities*tf.math.cumprod(1-probabilities, axis=1, exclusive=True)\n","    elif mode == 'recursive':\n","        batch_size = tf.shape(input=probabilities)[0]\n","        shifted_1mp_probabilities = tf.concat([tf.ones((batch_size, 1)),\\\n","            1 - probabilities[:, :-1]], 1)\n","        attention = probabilities*tf.transpose(a=tf.scan(lambda x, yz: tf.reshape(yz[0]*x + yz[1],\\\n","            (batch_size,)), [tf.transpose(a=shifted_1mp_probabilities),\\\n","                tf.transpose(a=attention_prev)], tf.zeros((batch_size,))))\n","    elif mode == 'parallel':\n","        cumprod_1mp_probabilities = tf.exp(tf.cumsum(tf.math.log(tf.clip_by_value(1-probabilities,\\\n","            1e-10, 1)), axis=1, exclusive=True))\n","        attention = probabilities*cumprod_1mp_probabilities*tf.cumsum(attention_prev/\\\n","            tf.clip_by_value(cumprod_1mp_probabilities, 1e-10, 1.), axis=1)\n","    else:\n","        raise ValueError(\"Mode must be 'hard', 'parallel' or 'recursive' \")\n","\n","    return attention \n","def deserialize(name, custom_objects=None):\n","    '''deserialize Keras Object'''\n","    return deserialize_keras_object(\n","        name,\n","        module_objects=globals(),\n","        custom_objects=custom_objects,\n","        printable_module_name='activation function')    \n","def softmax(inputs, axis=-1):\n","    '''softmax activation function\n","    # Arguments\n","        inputs = Input Tensor.\n","        axis = Integer, axis along which the softmax normalization is applied.\n","    # Returns\n","        Tensor, output of the softmax transformation\n","    '''\n","    return tf.nn.softmax(inputs, axis=axis)     \n","def _attention_score(dec_ht,\n","                     enc_hs,\n","                     attention_type,\n","                     weightwa=None,\n","                     weightua=None,\n","                     weightva=None):\n","    if attention_type == 'bahdanau':\n","        score = weightva(tf.nn.tanh(weightwa(dec_ht) + weightua(enc_hs)))\n","        score = tf.squeeze(score, [2])\n","    elif attention_type == 'dot':\n","        score = tf.matmul(dec_ht, enc_hs, transpose_b=True)\n","        score = tf.squeeze(score, 1)\n","    elif attention_type == 'general':\n","        score = weightwa(enc_hs)\n","        score = tf.matmul(dec_ht, score, transpose_b=True)\n","        score = tf.squeeze(score, 1)\n","    elif attention_type == 'concat':\n","        \n","        dec_ht = tf.tile(dec_ht, [1, enc_hs.shape[1], 1])\n","        score = weightva(tf.nn.tanh(weightwa(tf.concat((dec_ht, enc_hs), axis=-1))))\n","        score = tf.squeeze(score, 2)\n","    return score            "],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"IGXwjETXyTGp","executionInfo":{"status":"ok","timestamp":1617164652670,"user_tz":-330,"elapsed":1245,"user":{"displayName":"Nipun Agrawal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEnTNOPKYA066Ly1RwsNs4132AgrMH5fVw2BuEAw=s64","userId":"18285146514188802906"}}},"source":["class BahdanauAttention(Layer):\n","    '''\n","    BahdanauAttention:\n","    Implemented based on below paper\n","        https://arxiv.org/pdf/1409.0473.pdf\n","        attention_weights = probability_fn((Va * tanh(Wa*Ht+Ua*Hs+b))/sqrt(scaling_factor))\n","    # Arguments\n","        units = number of hidden units to use.\n","        probability_fn = probability function to get probabilities(weights for attention)\n","                         You can use 'softmax' or 'hardmax' or 'sparsemax' or any custom\n","                         function which takes input distribution and returns probability dist.\n","        dropout_rate = dropout for attention weights (between 0 and 1, 0 - no dropout).\n","        return_aweights = Bool, whether to return attention weights or not.\n","        scaling_factor = int/float to scale the score vector. default None=1\n","        weights_initializer = initializer for weight matrix\n","        bias_initializer = initializer for bias values\n","        weights_regularizer = Regularize the weights (U, W, V)\n","        bias_regularizer = Regularize the bias (b)\n","        weights_constraint = Constraint function applied to the weights\n","        bias_constraint = Constraint function applied to the bias\n","    # Returns\n","        context_vector = context vector after applying attention.\n","        attention_weights = attention weights only if `return_aweights=True`.\n","    # Inputs to the layer\n","        inputs = dictionary with keys \"enocderHs\", \"decoderHt\".\n","                enocderHs = all the encoder hidden states,\n","                            shape - (Batchsize, encoder_seq_len, enc_hidden_size)\n","                 decoderHt = hidden state of decoder at that timestep,\n","                            shape - (Batchsize, dec_hidden_size)\n","        mask = You can apply mask for padded values or any custom values\n","               while calculating attention.\n","               if you are giving mask for encoder and deocoder then you have\n","               to give a dict similar to inputs. (keys: enocderHs, decoderHt)\n","               else you can give only for enocoder normally.(one tensor)\n","               mask shape should be (Batchsize, encoder_seq_len)\n","    '''\n","    def __init__(self, units,\n","                 probability_fn='softmax',\n","                 dropout_rate=0,\n","                 return_aweights=False,\n","                 scaling_factor=None,\n","                 weights_initializer='he_normal',\n","                 bias_initializer='zeros',\n","                 weights_regularizer=None,\n","                 bias_regularizer=None,\n","                 weights_constraint=None,\n","                 bias_constraint=None,\n","                 **kwargs):\n","\n","        if 'name' not in kwargs:\n","            kwargs['name'] = \"\"\n","        super(BahdanauAttention, self).__init__(**kwargs)\n","\n","        self.units = units\n","        self.dropout_rate = dropout_rate\n","        self.return_aweights = return_aweights\n","        self.scaling_factor = scaling_factor\n","        self.probability_fn = get(probability_fn)\n","        self.weights_initializer = initializers.get(weights_initializer)\n","        self.bias_initializer = initializers.get(bias_initializer)\n","        self.weights_regularizer = regularizers.get(weights_regularizer)\n","        self.bias_regularizer = regularizers.get(bias_regularizer)\n","        self.weights_constraint = constraints.get(weights_constraint)\n","        self.bias_constraint = constraints.get(bias_constraint)\n","        self._wa = layers.Dense(self.units, use_bias=False,\\\n","            kernel_initializer=weights_initializer, bias_initializer=bias_initializer,\\\n","                kernel_regularizer=weights_regularizer, bias_regularizer=bias_regularizer,\\\n","                    kernel_constraint=weights_constraint, bias_constraint=bias_constraint,\\\n","                        name=self.name+\"Wa\")\n","        self._ua = layers.Dense(self.units,\\\n","            kernel_initializer=weights_initializer, bias_initializer=bias_initializer,\\\n","                kernel_regularizer=weights_regularizer, bias_regularizer=bias_regularizer,\\\n","                    kernel_constraint=weights_constraint, bias_constraint=bias_constraint,\\\n","                        name=self.name+\"Ua\")\n","        self._va = layers.Dense(1, use_bias=False, kernel_initializer=weights_initializer,\\\n","            kernel_regularizer=weights_regularizer, bias_regularizer=bias_regularizer,\\\n","                bias_initializer=bias_initializer, kernel_constraint=weights_constraint,\\\n","                    bias_constraint=bias_constraint, name=self.name+\"Va\")\n","        self.supports_masking = True\n","\n","\n","    def build(self, input_shape):\n","        '''build'''\n","        assert isinstance(input_shape, dict)\n","\n","        shape_en, shape_dc = input_shape['enocderHs'], input_shape['decoderHt']\n","\n","        assert len(shape_en) >= 3, \"Encoder Hiddenstates/output should be 3 dim or more \\\n","        ( B x T x H ), but got {} dim\".format(len(shape_en))\n","\n","        assert len(shape_dc) == 2, \"Decoder Hidden/output should be 2 \\\n","        dim (B x H), but got {} dim\".format(len(shape_dc))\n","\n","        self.built = True # pylint: disable=W0201\n","    def call(self, inputs, mask=None):\n","        '''call'''\n","        assert isinstance(inputs, dict)\n","\n","        if ('enocderHs' not in inputs.keys())or ('decoderHt' not in inputs.keys()):\n","            raise ValueError(\"Input to the layer must be a dict with \\\n","            keys=['enocderHs','decoderHt']\")\n","        if isinstance(mask, dict):\n","            mask_enc = mask.get('enocderHs', None)\n","            mask_dec = mask.get('decoderHt', None)\n","        else:\n","            mask_enc = mask\n","            mask_dec = None\n","\n","        enc_out, dec_prev_hs = tf.cast(inputs['enocderHs'], tf.float32), \\\n","            tf.cast(inputs['decoderHt'], tf.float32)\n","        if mask_dec is not None:\n","            dec_prev_hs = dec_prev_hs * tf.cast(mask_dec, dec_prev_hs.dtype)\n","        if mask_enc is not None:\n","            enc_out = enc_out * tf.cast(tf.expand_dims(mask_enc, 2), enc_out.dtype)\n","\n","        # decprev_hs - Decoder hidden shape == (batch_size, hidden size)\n","        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n","        dec_hidden_with_time_axis = tf.expand_dims(dec_prev_hs, 1)\n","\n","        # score shape == (batch_size, max_length)\n","        score = _attention_score(dec_ht=dec_hidden_with_time_axis, enc_hs=enc_out,\\\n","                    attention_type='concat', weightwa=self._wa,\\\n","                        weightua=self._ua, weightva=self._va)\n","\n","        if self.scaling_factor is not None:\n","            score = score/tf.sqrt(self.scaling_factor)\n","\n","        #if mask_enc is not None:\n","            #score = score + (tf.cast(tf.math.equal(mask_enc, False), score.dtype)*-1e9)\n","\n","        # attention_weights shape == (batch_size, max_length)\n","        attention_weights = self.probability_fn(score, axis=-1)\n","        #(batch_size, 1, max_length)\n","        attention_weights = tf.expand_dims(attention_weights, 1)\n","\n","        if self.dropout_rate != 0:\n","            attention_weights = tf.nn.dropout(x=attention_weights, rate=self.dropout_rate)\n","\n","        #context_vector shape (batch_size, hidden_size)\n","        context_vector = tf.matmul(attention_weights, enc_out)\n","        context_vector = tf.squeeze(context_vector, 1, name=\"context_vector\")\n","\n","        if self.return_aweights:\n","            return context_vector, tf.squeeze(attention_weights, 1, name='attention_weights')\n","        return context_vector\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"3S5e2SEEZUVp","executionInfo":{"status":"ok","timestamp":1617164659714,"user_tz":-330,"elapsed":1793,"user":{"displayName":"Nipun Agrawal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEnTNOPKYA066Ly1RwsNs4132AgrMH5fVw2BuEAw=s64","userId":"18285146514188802906"}}},"source":["class MonotonicBahdanauAttention(Layer):\n","    '''\n","    MonotonicBahdanauAttention\n","    Implemented based on below paper\n","        https://arxiv.org/pdf/1704.00784.pdf\n","    # Arguments\n","        units = number of hidden units to use.\n","        mode = How to compute the attention distribution.\n","              Must be one of 'recursive', 'parallel', or 'hard'.\n","              - 'recursive' uses tf.scan to recursively compute the distribution.\n","              This is slowest but is exact, general, and does not suffer from\n","              numerical instabilities.\n","              - 'parallel' uses parallelized cumulative-sum and cumulative-product\n","              operations to compute a closed-form solution to the recurrence relation\n","              defining the attention distribution.  This makes it more efficient than 'recursive',\n","              but it requires numerical checks which make the distribution non-exact.\n","              This can be a problem in particular when max_length is long and/or\n","              probabilities has entries very close to 0 or 1.\n","              - 'hard' requires that  the probabilities in p_choose_i are all either 0 or 1,\n","              and subsequently uses a more efficient and exact solution.\n","        return_aweights = Bool, whether to return attention weights or not.\n","        scaling_factor = int/float to scale the score vector. default None=1\n","        noise_std = standard deviation of noise which will be added before\n","                    applying sigmoid function.(pre-sigmoid noise). If it is 0 or\n","                    mode=\"hard\", we won't add any noise.\n","        weights_initializer = initializer for weight matrix\n","        weights_regularizer = Regularize the weights\n","        weights_constraint = Constraint function applied to the weights\n","    # Returns\n","        context_vector = context vector after applying attention.\n","        attention_weights = attention weights only if `return_aweights=True`.\n","    # Inputs to the layer\n","        inputs = dictionary with keys \"enocderHs\", \"decoderHt\", \"prevAttention\".\n","                enocderHs = all the encoder hidden states,\n","                            shape - (Batchsize, encoder_seq_len, enc_hidden_size)\n","                decoderHt = hidden state of decoder at that timestep,\n","                            shape - (Batchsize, dec_hidden_size)\n","                prevAttention = Previous probability distribution of attention\n","                                (previous attention weights)\n","        mask = You can apply mask for padded values or any custom values\n","               while calculating attention.\n","               if you are giving mask for encoder and deocoder then you have\n","               to give a dict similar to inputs. (keys: enocderHs, decoderHt)\n","               else you can give only for enocoder normally.(one tensor)\n","               mask shape should be (Batchsize, encoder_seq_len)\n","    '''\n"," \n","    def __init__(self, units,\n","                 mode='parallel',\n","                 return_aweights=False,\n","                 scaling_factor=None,\n","                 noise_std=0,\n","                 weights_initializer='he_normal',\n","                 bias_initializer='zeros',\n","                 weights_regularizer=None,\n","                 bias_regularizer=None,\n","                 weights_constraint=None,\n","                 bias_constraint=None,\n","                 **kwargs):\n","        if 'name' not in kwargs:\n","            kwargs['name'] = \"\"\n","        super(MonotonicBahdanauAttention, self).__init__(**kwargs)\n","        self.units = units\n","        self.mode = mode\n","        self.return_aweights = return_aweights\n","        self.scaling_factor = scaling_factor\n","        self.noise_std = noise_std\n","        self.weights_initializer = initializers.get(weights_initializer)\n","        self.bias_initializer = initializers.get(bias_initializer)\n","        self.weights_regularizer = regularizers.get(weights_regularizer)\n","        self.bias_regularizer = regularizers.get(bias_regularizer)\n","        self.weights_constraint = constraints.get(weights_constraint)\n","        self.bias_constraint = constraints.get(bias_constraint)\n","        self._wa = layers.Dense(self.units, use_bias=False,\\\n","            kernel_initializer=weights_initializer, bias_initializer=bias_initializer,\\\n","                kernel_regularizer=weights_regularizer, bias_regularizer=bias_regularizer,\\\n","                    kernel_constraint=weights_constraint, bias_constraint=bias_constraint,\\\n","                        name=self.name+\"Wa\")\n","        self._ua = layers.Dense(self.units,\\\n","            kernel_initializer=weights_initializer, bias_initializer=bias_initializer,\\\n","                kernel_regularizer=weights_regularizer, bias_regularizer=bias_regularizer,\\\n","                    kernel_constraint=weights_constraint, bias_constraint=bias_constraint,\\\n","                        name=self.name+\"Ua\")\n","        self._va = layers.Dense(1, use_bias=False, kernel_initializer=weights_initializer,\\\n","            kernel_regularizer=weights_regularizer, bias_regularizer=bias_regularizer,\\\n","                bias_initializer=bias_initializer, kernel_constraint=weights_constraint,\\\n","                    bias_constraint=bias_constraint, name=self.name+\"Va\")\n","        self.supports_masking = True\n"," \n","    def build(self, input_shape):\n","        '''build'''\n","        assert isinstance(input_shape, dict)\n"," \n","        shape_en, shape_dc = input_shape['enocderHs'], input_shape['decoderHt']\n","        shape_prob = input_shape['prevAttention']\n","        \n"," \n","        assert len(shape_en) >= 3, \"Encoder Hiddenstates/output should be 3 dim or more \\\n","        ( B x T x H ), but got {} dim\".format(len(shape_en))\n"," \n","        assert len(shape_dc) == 2, \"Decoder Hidden/output should be 2 \\\n","        dim (B x H), but got {} dim\".format(len(shape_dc))\n"," \n","        assert len(shape_prob) == 2, \"Previous probability should be 2 \\\n","        dim (B x H), but got {} dim\".format(len(shape_prob))\n"," \n","        self.built = True # pylint: disable=W0201\n"," \n","    def call(self, inputs, mask=None, training=True):\n","        '''call'''\n","        assert isinstance(inputs, dict)\n","       \n"," \n","        if ('enocderHs' not in inputs.keys())or ('decoderHt' not in inputs.keys()\\\n","            or 'prevAttention' not in inputs.keys()):\n","            raise ValueError(\"Input to the layer must be a dict with \\\n","            keys=['enocderHs','decoderHt', 'prevAttention']\")\n"," \n","        if isinstance(mask, dict):\n","            mask_enc = mask.get('enocderHs', None)\n","            mask_dec = mask.get('decoderHt', None)\n","        else:\n","            mask_enc = mask\n","            mask_dec = None\n","        enc_out, dec_prev_hs = tf.cast(inputs['enocderHs'], tf.float32), \\\n","            tf.cast(inputs['decoderHt'], tf.float32)\n"," \n","        prev_attention = inputs['prevAttention']\n","      \n"," \n","        if mask_dec is not None:\n","            dec_prev_hs = dec_prev_hs * tf.cast(mask_dec, dec_prev_hs.dtype)\n","        if mask_enc is not None:\n","            enc_out = enc_out * tf.cast(tf.expand_dims(mask_enc, 2), enc_out.dtype)\n"," \n","        # decprev_hs - Decoder hidden shape == (batch_size, hidden size)\n","        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n","        dec_hidden_with_time_axis = tf.expand_dims(dec_prev_hs, 1)\n","       \n"," \n","        # score shape == (batch_size, max_length)\n","        score = _attention_score(dec_ht=dec_hidden_with_time_axis, enc_hs=enc_out,\\\n","                    attention_type='concat', weightwa=self._wa,\\\n","                        weightua=self._ua, weightva=self._va)\n"," \n","        if self.scaling_factor is not None:\n","            score = score/tf.sqrt(self.scaling_factor)\n"," \n","        if training:\n","            if self.noise_std > 0:\n","                random_noise = tf.random.normal(shape=tf.shape(input=score), mean=0,\\\n","                    stddev=self.noise_std, dtype=score.dtype, seed=self.seed)\n","                score = score + random_noise\n"," \n","        if mask_enc is not None:\n","            score = score + (tf.cast(tf.math.equal(mask_enc, False), score.dtype)*-1e9)\n"," \n","        if self.mode == 'hard':\n","            probabilities = tf.cast(score > 0, score.dtype)\n","        else:\n","            probabilities = tf.sigmoid(score)\n"," \n","        attention_weights = _monotonic_attetion(probabilities, prev_attention, self.mode)\n","        attention_weights = tf.expand_dims(attention_weights, 1)\n"," \n","        #context_vector shape (batch_size, hidden_size)\n","        context_vector = tf.matmul(attention_weights, enc_out)\n","        context_vector = tf.squeeze(context_vector, 1, name=\"context_vector\")\n"," \n","        if self.return_aweights:\n","         \n","            return context_vector, tf.squeeze(attention_weights, 1, name='attention_weights')\n","        return context_vector\n"," \n","    def compute_output_shape(self, input_shape):\n","        '''compute output shape'''\n","        assert isinstance(input_shape, dict)\n","        shape_en = input_shape['enocderHs']\n","        if self.return_aweights:\n","            output_shape = [(shape_en[0], shape_en[2]), (shape_en[0], shape_en[1])]\n","            return output_shape\n","        output_shape = shape_en[0], shape_en[2]\n","        return output_shape\n"," \n","    def get_config(self):\n","        '''Config'''\n","        config = {'units': self.units,\n","                  'mode':self.mode,\n","                  'return_aweights' : self.return_aweights,\n","                  'scaling_factor' : self.scaling_factor,\n","                  'noise_std':self.noise_std,\n","                  'weights_initializer' : initializers.serialize(self.weights_initializer),\n","                  'bias_initializer' : initializers.serialize(self.bias_initializer),\n","                  'weights_regularizer' : regularizers.serialize(self.weights_regularizer),\n","                  'bias_regularizer' : regularizers.serialize(self.bias_regularizer),\n","                  'weights_constraint' : constraints.serialize(self.weights_constraint),\n","                  'bias_constraint': constraints.serialize(self.bias_constraint)}\n","        base_config = super(MonotonicBahdanauAttention, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"MMqs-kVVZ83f","executionInfo":{"status":"ok","timestamp":1617168498065,"user_tz":-330,"elapsed":861,"user":{"displayName":"Nipun Agrawal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEnTNOPKYA066Ly1RwsNs4132AgrMH5fVw2BuEAw=s64","userId":"18285146514188802906"}}},"source":["def encoder():\n","    input_encoder = Input(shape=(ENCODER_SEQ_LEN,), name='encoder_input')\n","\n","    x_embedded = Embedding(input_dim=EN_VOCAB_SIZE, output_dim=300, input_length=ENCODER_SEQ_LEN,\n","                           mask_zero=False, name=\"embedding_layer_encoder\",weights=[encoder_embedding])(input_encoder)\n","    print(x_embedded.shape)\n","    gru_output, gru_state= GRU(units, return_state=True, return_sequences=True, name=\"Encoder_GRU\")(x_embedded)\n","    return Model(inputs=input_encoder, outputs=[gru_output, gru_state])"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"TvD-JYmgaDtC","executionInfo":{"status":"ok","timestamp":1617168505141,"user_tz":-330,"elapsed":934,"user":{"displayName":"Nipun Agrawal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEnTNOPKYA066Ly1RwsNs4132AgrMH5fVw2BuEAw=s64","userId":"18285146514188802906"}}},"source":[" \n","def OneStepDecoder():\n","    \n","    #inputs\n","    input_decoder = Input(shape=(1,),name=\"dec_input\")\n","    \n","    input_state = Input(shape=(units,),name=\"Dec_states\")\n","    \n","    encoder_outputs = Input(shape=(ENCODER_SEQ_LEN, units,),name=\"EncoderStates\")\n","    prevAttention=Input(shape=(38,),name=\"PreviousAttention\")\n","    \n","    #mask_encoder = Input(shape=(ENCODER_SEQ_LEN,), name='mask_encoder')\n","    \n","    # Embedding layers\n","    dec_embdd = Embedding(input_dim=DE_VOCAB_SIZE, output_dim=300,\n","                      input_length=1,mask_zero=False, name=\"Decoder_Embedding_layer\",weights=[decoder_embedding])(input_decoder)\n","    \n","    \n","    z_attention = dict()\n","    z_attention['enocderHs'] = encoder_outputs\n","    z_attention['decoderHt'] = input_state\n","    z_attention['prevAttention']=prevAttention\n","    print(z_attention['prevAttention'].shape)\n","    \n","    context_vector,prev_Attention = MonotonicBahdanauAttention(units=20, name='parallel',return_aweights=True)(z_attention)\n","    print(prevAttention)\n","    \n","    concat = concatenate([tf.cast(tf.expand_dims(context_vector, 1), dec_embdd.dtype), dec_embdd],name=\"concat\")\n","    \n","    decoder_output, Decoder_state = GRU(units=units,return_state=True,name=\"DecGRU\")(concat, initial_state=input_state)\n","    \n","    output = Dense(units=DE_VOCAB_SIZE,activation=\"softmax\",name=\"DenseOut\")(decoder_output)\n","    print(output.shape)\n","    return Model(inputs=[input_decoder, input_state, encoder_outputs,prevAttention],outputs=[output, Decoder_state,prev_Attention],name=\"OneStepDecoder\")"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"RbdeSqkNaRSn","executionInfo":{"status":"ok","timestamp":1617164684576,"user_tz":-330,"elapsed":1171,"user":{"displayName":"Nipun Agrawal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEnTNOPKYA066Ly1RwsNs4132AgrMH5fVw2BuEAw=s64","userId":"18285146514188802906"}}},"source":[" \n","def encoder_decoder():\n","    \n","    encoder_input = Input(shape=(ENCODER_SEQ_LEN,), name='encoder_input_final')\n","    decoder_input = Input(shape=(DECODER_SEQ_LEN,), name=\"Decoder_input_final\")\n","    \n","    enc = encoder()\n"," \n","    outputs_encoder, encoder_state = enc(encoder_input)\n","    print(outputs_encoder.shape)\n","    #batch size\n","    batch_size=4\n","    max_seq=38\n","    \n","    all_outputs= []\n","    #intialisation of numpy zero matrix for previous attention\n","    prevAttention=np.zeros((batch_size,max_seq))\n","    #print(prevAttention.shape)\n"," \n","    prevAttention[:,0]=1\n","    decoder_one_att = OneStepDecoder()\n"," \n","    \n","    \n","    print(prevAttention)\n","    \n","    for timestep in range(DECODER_SEQ_LEN):\n","        \n","        inputs = Lambda(lambda x: x[:,timestep:timestep+1])(decoder_input)\n","        \n","        output, encoder_state,prevAttention = decoder_one_att([inputs, encoder_state, outputs_encoder,prevAttention])\n","        \n","        output = Lambda(lambda x:  tf.expand_dims(x, 1))(output)\n","        \n","        all_outputs.append(output)\n","        \n","  \n","    #decoder_outputs = Lambda(lambda x: tf.keras.backend.concatenate(all_outputs,1))(all_outputs)\n","    decoder_outputs=tf.concat(all_outputs,1)\n","    print(decoder_outputs.shape)\n","    \n","    return Model(inputs=[encoder_input, decoder_input], outputs=decoder_outputs)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"doMLyhGnaU_m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617168520507,"user_tz":-330,"elapsed":11247,"user":{"displayName":"Nipun Agrawal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEnTNOPKYA066Ly1RwsNs4132AgrMH5fVw2BuEAw=s64","userId":"18285146514188802906"}},"outputId":"6129b438-298e-4074-d6e7-6c46a054541f"},"source":["EC=encoder_decoder()"],"execution_count":15,"outputs":[{"output_type":"stream","text":["(None, 38, 300)\n","(None, 38, 100)\n","(None, 38)\n","KerasTensor(type_spec=TensorSpec(shape=(None, 38), dtype=tf.float32, name='PreviousAttention'), name='PreviousAttention', description=\"created by layer 'PreviousAttention'\")\n","(None, 3040)\n","[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n","(4, 39, 3040)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1OZJg5kf_KMW","executionInfo":{"status":"ok","timestamp":1616820907532,"user_tz":-330,"elapsed":983,"user":{"displayName":"Nipun Agrawal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEnTNOPKYA066Ly1RwsNs4132AgrMH5fVw2BuEAw=s64","userId":"18285146514188802906"}},"outputId":"c5154197-fc06-4264-806e-5d426dd07277"},"source":["EC.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","Decoder_input_final (InputLayer [(None, 39)]         0                                            \n","__________________________________________________________________________________________________\n","encoder_input_final (InputLayer [(None, 38)]         0                                            \n","__________________________________________________________________________________________________\n","lambda (Lambda)                 (None, 1)            0           Decoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","model (Functional)              [(None, 38, 100), (N 1231200     encoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","OneStepDecoder (Functional)     [(None, 3040), (None 1373660     lambda[0][0]                     \n","                                                                 model[0][1]                      \n","                                                                 model[0][0]                      \n","                                                                 lambda_2[0][0]                   \n","                                                                 OneStepDecoder[0][1]             \n","                                                                 model[0][0]                      \n","                                                                 OneStepDecoder[0][2]             \n","                                                                 lambda_4[0][0]                   \n","                                                                 OneStepDecoder[1][1]             \n","                                                                 model[0][0]                      \n","                                                                 OneStepDecoder[1][2]             \n","                                                                 lambda_6[0][0]                   \n","                                                                 OneStepDecoder[2][1]             \n","                                                                 model[0][0]                      \n","                                                                 OneStepDecoder[2][2]             \n","                                                                 lambda_8[0][0]                   \n","                                                                 OneStepDecoder[3][1]             \n","                                                                 model[0][0]                      \n","                                                                 OneStepDecoder[3][2]             \n","                                                                 lambda_10[0][0]                  \n","                                                                 OneStepDecoder[4][1]             \n","                                                                 model[0][0]                      \n","                                                                 OneStepDecoder[4][2]             \n","                                                                 lambda_12[0][0]                  \n","                                                                 OneStepDecoder[5][1]             \n","                                                                 model[0][0]                      \n","                                                                 OneStepDecoder[5][2]             \n","                                                                 lambda_14[0][0]                  \n","                                                                 OneStepDecoder[6][1]             \n","                                                                 model[0][0]                      \n","                                                                 OneStepDecoder[6][2]             \n","                                                                 lambda_16[0][0]                  \n","                                                                 OneStepDecoder[7][1]             \n","                                                                 model[0][0]                      \n","                                                                 OneStepDecoder[7][2]             \n","                                                                 lambda_18[0][0]                  \n","                                                                 OneStepDecoder[8][1]             \n","                                                                 model[0][0]                      \n","                                                                 OneStepDecoder[8][2]             \n","                                                                 lambda_20[0][0]                  \n","                                                                 OneStepDecoder[9][1]             \n","                                                                 model[0][0]                      \n","                                                                 OneStepDecoder[9][2]             \n","                                                                 lambda_22[0][0]                  \n","                                                                 OneStepDecoder[10][1]            \n","                                                                 model[0][0]                      \n","                                                                 OneStepDecoder[10][2]            \n","                                                                 lambda_24[0][0]                  \n","                                                                 OneStepDecoder[11][1]            \n","                                                                 model[0][0]                      \n","                                                                 OneStepDecoder[11][2]            \n","                                                                 lambda_26[0][0]                  \n","                                                                 OneStepDecoder[12][1]            \n","                                                                 model[0][0]                      \n","                                                                 OneStepDecoder[12][2]            \n","                                                                 lambda_28[0][0]                  \n","                                                                 OneStepDecoder[13][1]            \n","                                                                 model[0][0]                      \n","                                                                 OneStepDecoder[13][2]            \n","                                                                 lambda_30[0][0]                  \n","                                                                 OneStepDecoder[14][1]            \n","                                                                 model[0][0]                      \n","                                                                 OneStepDecoder[14][2]            \n","                                                                 lambda_32[0][0]                  \n","                                                                 OneStepDecoder[15][1]            \n","                                                                 model[0][0]                      \n","                                                                 OneStepDecoder[15][2]            \n","                                                                 lambda_34[0][0]                  \n","                                                                 OneStepDecoder[16][1]            \n","                                                                 model[0][0]                      \n","                                                                 OneStepDecoder[16][2]            \n","                                                                 lambda_36[0][0]                  \n","                                                                 OneStepDecoder[17][1]            \n","                                                                 model[0][0]                      \n","                                                                 OneStepDecoder[17][2]            \n","                                                                 lambda_38[0][0]                  \n","                                                                 OneStepDecoder[18][1]            \n","                                                                 model[0][0]                      \n","                                                                 OneStepDecoder[18][2]            \n","                                                                 lambda_40[0][0]                  \n","                                                                 OneStepDecoder[19][1]            \n","                                                                 model[0][0]                      \n","                                                                 OneStepDecoder[19][2]            \n","                                                                 lambda_42[0][0]                  \n","                                                                 OneStepDecoder[20][1]            \n","                                                                 model[0][0]                      \n","                                                                 OneStepDecoder[20][2]            \n","                                                                 lambda_44[0][0]                  \n","                                                                 OneStepDecoder[21][1]            \n","                                                                 model[0][0]                      \n","                                                                 OneStepDecoder[21][2]            \n","                                                                 lambda_46[0][0]                  \n","                                                                 OneStepDecoder[22][1]            \n","                                                                 model[0][0]                      \n","                                                                 OneStepDecoder[22][2]            \n","                                                                 lambda_48[0][0]                  \n","                                                                 OneStepDecoder[23][1]            \n","                                                                 model[0][0]                      \n","                                                                 OneStepDecoder[23][2]            \n","                                                                 lambda_50[0][0]                  \n","                                                                 OneStepDecoder[24][1]            \n","                                                                 model[0][0]                      \n","                                                                 OneStepDecoder[24][2]            \n","                                                                 lambda_52[0][0]                  \n","                                                                 OneStepDecoder[25][1]            \n","                                                                 model[0][0]                      \n","                                                                 OneStepDecoder[25][2]            \n","                                                                 lambda_54[0][0]                  \n","                                                                 OneStepDecoder[26][1]            \n","                                                                 model[0][0]                      \n","                                                                 OneStepDecoder[26][2]            \n","                                                                 lambda_56[0][0]                  \n","                                                                 OneStepDecoder[27][1]            \n","                                                                 model[0][0]                      \n","                                                                 OneStepDecoder[27][2]            \n","                                                                 lambda_58[0][0]                  \n","                                                                 OneStepDecoder[28][1]            \n","                                                                 model[0][0]                      \n","                                                                 OneStepDecoder[28][2]            \n","                                                                 lambda_60[0][0]                  \n","                                                                 OneStepDecoder[29][1]            \n","                                                                 model[0][0]                      \n","                                                                 OneStepDecoder[29][2]            \n","                                                                 lambda_62[0][0]                  \n","                                                                 OneStepDecoder[30][1]            \n","                                                                 model[0][0]                      \n","                                                                 OneStepDecoder[30][2]            \n","                                                                 lambda_64[0][0]                  \n","                                                                 OneStepDecoder[31][1]            \n","                                                                 model[0][0]                      \n","                                                                 OneStepDecoder[31][2]            \n","                                                                 lambda_66[0][0]                  \n","                                                                 OneStepDecoder[32][1]            \n","                                                                 model[0][0]                      \n","                                                                 OneStepDecoder[32][2]            \n","                                                                 lambda_68[0][0]                  \n","                                                                 OneStepDecoder[33][1]            \n","                                                                 model[0][0]                      \n","                                                                 OneStepDecoder[33][2]            \n","                                                                 lambda_70[0][0]                  \n","                                                                 OneStepDecoder[34][1]            \n","                                                                 model[0][0]                      \n","                                                                 OneStepDecoder[34][2]            \n","                                                                 lambda_72[0][0]                  \n","                                                                 OneStepDecoder[35][1]            \n","                                                                 model[0][0]                      \n","                                                                 OneStepDecoder[35][2]            \n","                                                                 lambda_74[0][0]                  \n","                                                                 OneStepDecoder[36][1]            \n","                                                                 model[0][0]                      \n","                                                                 OneStepDecoder[36][2]            \n","                                                                 lambda_76[0][0]                  \n","                                                                 OneStepDecoder[37][1]            \n","                                                                 model[0][0]                      \n","                                                                 OneStepDecoder[37][2]            \n","__________________________________________________________________________________________________\n","lambda_2 (Lambda)               (None, 1)            0           Decoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","lambda_4 (Lambda)               (None, 1)            0           Decoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","lambda_6 (Lambda)               (None, 1)            0           Decoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","lambda_8 (Lambda)               (None, 1)            0           Decoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","lambda_10 (Lambda)              (None, 1)            0           Decoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","lambda_12 (Lambda)              (None, 1)            0           Decoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","lambda_14 (Lambda)              (None, 1)            0           Decoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","lambda_16 (Lambda)              (None, 1)            0           Decoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","lambda_18 (Lambda)              (None, 1)            0           Decoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","lambda_20 (Lambda)              (None, 1)            0           Decoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","lambda_22 (Lambda)              (None, 1)            0           Decoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","lambda_24 (Lambda)              (None, 1)            0           Decoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","lambda_26 (Lambda)              (None, 1)            0           Decoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","lambda_28 (Lambda)              (None, 1)            0           Decoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","lambda_30 (Lambda)              (None, 1)            0           Decoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","lambda_32 (Lambda)              (None, 1)            0           Decoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","lambda_34 (Lambda)              (None, 1)            0           Decoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","lambda_36 (Lambda)              (None, 1)            0           Decoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","lambda_38 (Lambda)              (None, 1)            0           Decoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","lambda_40 (Lambda)              (None, 1)            0           Decoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","lambda_42 (Lambda)              (None, 1)            0           Decoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","lambda_44 (Lambda)              (None, 1)            0           Decoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","lambda_46 (Lambda)              (None, 1)            0           Decoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","lambda_48 (Lambda)              (None, 1)            0           Decoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","lambda_50 (Lambda)              (None, 1)            0           Decoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","lambda_52 (Lambda)              (None, 1)            0           Decoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","lambda_54 (Lambda)              (None, 1)            0           Decoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","lambda_56 (Lambda)              (None, 1)            0           Decoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","lambda_58 (Lambda)              (None, 1)            0           Decoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","lambda_60 (Lambda)              (None, 1)            0           Decoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","lambda_62 (Lambda)              (None, 1)            0           Decoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","lambda_64 (Lambda)              (None, 1)            0           Decoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","lambda_66 (Lambda)              (None, 1)            0           Decoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","lambda_68 (Lambda)              (None, 1)            0           Decoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","lambda_70 (Lambda)              (None, 1)            0           Decoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","lambda_72 (Lambda)              (None, 1)            0           Decoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","lambda_74 (Lambda)              (None, 1)            0           Decoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","lambda_76 (Lambda)              (None, 1)            0           Decoder_input_final[0][0]        \n","__________________________________________________________________________________________________\n","lambda_1 (Lambda)               (64, 1, 3040)        0           OneStepDecoder[0][0]             \n","__________________________________________________________________________________________________\n","lambda_3 (Lambda)               (64, 1, 3040)        0           OneStepDecoder[1][0]             \n","__________________________________________________________________________________________________\n","lambda_5 (Lambda)               (64, 1, 3040)        0           OneStepDecoder[2][0]             \n","__________________________________________________________________________________________________\n","lambda_7 (Lambda)               (64, 1, 3040)        0           OneStepDecoder[3][0]             \n","__________________________________________________________________________________________________\n","lambda_9 (Lambda)               (64, 1, 3040)        0           OneStepDecoder[4][0]             \n","__________________________________________________________________________________________________\n","lambda_11 (Lambda)              (64, 1, 3040)        0           OneStepDecoder[5][0]             \n","__________________________________________________________________________________________________\n","lambda_13 (Lambda)              (64, 1, 3040)        0           OneStepDecoder[6][0]             \n","__________________________________________________________________________________________________\n","lambda_15 (Lambda)              (64, 1, 3040)        0           OneStepDecoder[7][0]             \n","__________________________________________________________________________________________________\n","lambda_17 (Lambda)              (64, 1, 3040)        0           OneStepDecoder[8][0]             \n","__________________________________________________________________________________________________\n","lambda_19 (Lambda)              (64, 1, 3040)        0           OneStepDecoder[9][0]             \n","__________________________________________________________________________________________________\n","lambda_21 (Lambda)              (64, 1, 3040)        0           OneStepDecoder[10][0]            \n","__________________________________________________________________________________________________\n","lambda_23 (Lambda)              (64, 1, 3040)        0           OneStepDecoder[11][0]            \n","__________________________________________________________________________________________________\n","lambda_25 (Lambda)              (64, 1, 3040)        0           OneStepDecoder[12][0]            \n","__________________________________________________________________________________________________\n","lambda_27 (Lambda)              (64, 1, 3040)        0           OneStepDecoder[13][0]            \n","__________________________________________________________________________________________________\n","lambda_29 (Lambda)              (64, 1, 3040)        0           OneStepDecoder[14][0]            \n","__________________________________________________________________________________________________\n","lambda_31 (Lambda)              (64, 1, 3040)        0           OneStepDecoder[15][0]            \n","__________________________________________________________________________________________________\n","lambda_33 (Lambda)              (64, 1, 3040)        0           OneStepDecoder[16][0]            \n","__________________________________________________________________________________________________\n","lambda_35 (Lambda)              (64, 1, 3040)        0           OneStepDecoder[17][0]            \n","__________________________________________________________________________________________________\n","lambda_37 (Lambda)              (64, 1, 3040)        0           OneStepDecoder[18][0]            \n","__________________________________________________________________________________________________\n","lambda_39 (Lambda)              (64, 1, 3040)        0           OneStepDecoder[19][0]            \n","__________________________________________________________________________________________________\n","lambda_41 (Lambda)              (64, 1, 3040)        0           OneStepDecoder[20][0]            \n","__________________________________________________________________________________________________\n","lambda_43 (Lambda)              (64, 1, 3040)        0           OneStepDecoder[21][0]            \n","__________________________________________________________________________________________________\n","lambda_45 (Lambda)              (64, 1, 3040)        0           OneStepDecoder[22][0]            \n","__________________________________________________________________________________________________\n","lambda_47 (Lambda)              (64, 1, 3040)        0           OneStepDecoder[23][0]            \n","__________________________________________________________________________________________________\n","lambda_49 (Lambda)              (64, 1, 3040)        0           OneStepDecoder[24][0]            \n","__________________________________________________________________________________________________\n","lambda_51 (Lambda)              (64, 1, 3040)        0           OneStepDecoder[25][0]            \n","__________________________________________________________________________________________________\n","lambda_53 (Lambda)              (64, 1, 3040)        0           OneStepDecoder[26][0]            \n","__________________________________________________________________________________________________\n","lambda_55 (Lambda)              (64, 1, 3040)        0           OneStepDecoder[27][0]            \n","__________________________________________________________________________________________________\n","lambda_57 (Lambda)              (64, 1, 3040)        0           OneStepDecoder[28][0]            \n","__________________________________________________________________________________________________\n","lambda_59 (Lambda)              (64, 1, 3040)        0           OneStepDecoder[29][0]            \n","__________________________________________________________________________________________________\n","lambda_61 (Lambda)              (64, 1, 3040)        0           OneStepDecoder[30][0]            \n","__________________________________________________________________________________________________\n","lambda_63 (Lambda)              (64, 1, 3040)        0           OneStepDecoder[31][0]            \n","__________________________________________________________________________________________________\n","lambda_65 (Lambda)              (64, 1, 3040)        0           OneStepDecoder[32][0]            \n","__________________________________________________________________________________________________\n","lambda_67 (Lambda)              (64, 1, 3040)        0           OneStepDecoder[33][0]            \n","__________________________________________________________________________________________________\n","lambda_69 (Lambda)              (64, 1, 3040)        0           OneStepDecoder[34][0]            \n","__________________________________________________________________________________________________\n","lambda_71 (Lambda)              (64, 1, 3040)        0           OneStepDecoder[35][0]            \n","__________________________________________________________________________________________________\n","lambda_73 (Lambda)              (64, 1, 3040)        0           OneStepDecoder[36][0]            \n","__________________________________________________________________________________________________\n","lambda_75 (Lambda)              (64, 1, 3040)        0           OneStepDecoder[37][0]            \n","__________________________________________________________________________________________________\n","lambda_77 (Lambda)              (64, 1, 3040)        0           OneStepDecoder[38][0]            \n","__________________________________________________________________________________________________\n","tf.concat (TFOpLambda)          (64, 39, 3040)       0           lambda_1[0][0]                   \n","                                                                 lambda_3[0][0]                   \n","                                                                 lambda_5[0][0]                   \n","                                                                 lambda_7[0][0]                   \n","                                                                 lambda_9[0][0]                   \n","                                                                 lambda_11[0][0]                  \n","                                                                 lambda_13[0][0]                  \n","                                                                 lambda_15[0][0]                  \n","                                                                 lambda_17[0][0]                  \n","                                                                 lambda_19[0][0]                  \n","                                                                 lambda_21[0][0]                  \n","                                                                 lambda_23[0][0]                  \n","                                                                 lambda_25[0][0]                  \n","                                                                 lambda_27[0][0]                  \n","                                                                 lambda_29[0][0]                  \n","                                                                 lambda_31[0][0]                  \n","                                                                 lambda_33[0][0]                  \n","                                                                 lambda_35[0][0]                  \n","                                                                 lambda_37[0][0]                  \n","                                                                 lambda_39[0][0]                  \n","                                                                 lambda_41[0][0]                  \n","                                                                 lambda_43[0][0]                  \n","                                                                 lambda_45[0][0]                  \n","                                                                 lambda_47[0][0]                  \n","                                                                 lambda_49[0][0]                  \n","                                                                 lambda_51[0][0]                  \n","                                                                 lambda_53[0][0]                  \n","                                                                 lambda_55[0][0]                  \n","                                                                 lambda_57[0][0]                  \n","                                                                 lambda_59[0][0]                  \n","                                                                 lambda_61[0][0]                  \n","                                                                 lambda_63[0][0]                  \n","                                                                 lambda_65[0][0]                  \n","                                                                 lambda_67[0][0]                  \n","                                                                 lambda_69[0][0]                  \n","                                                                 lambda_71[0][0]                  \n","                                                                 lambda_73[0][0]                  \n","                                                                 lambda_75[0][0]                  \n","                                                                 lambda_77[0][0]                  \n","==================================================================================================\n","Total params: 2,604,860\n","Trainable params: 2,604,860\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eN-GjAplxtpp"},"source":["With Mask Values."]},{"cell_type":"code","metadata":{"id":"YDlxyFRL_Q_a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617168478016,"user_tz":-330,"elapsed":3731588,"user":{"displayName":"Nipun Agrawal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEnTNOPKYA066Ly1RwsNs4132AgrMH5fVw2BuEAw=s64","userId":"18285146514188802906"}},"outputId":"db0fb1cf-d5e1-4d6f-d1c0-8d0abc2fb224"},"source":["EC.compile(optimizer=tf.keras.optimizers.Adam(),loss=tf.keras.losses.sparse_categorical_crossentropy)\n","EC.fit(x=[x,y],y=z,epochs=50,verbose=1,batch_size=4,validation_data=([encoder_test,decoder_inp_test],decoder_out_test))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Epoch 1/50\n","492/492 [==============================] - 216s 205ms/step - loss: 7.8517 - val_loss: 7.3894\n","Epoch 2/50\n","492/492 [==============================] - 72s 147ms/step - loss: 7.2290 - val_loss: 6.8247\n","Epoch 3/50\n","492/492 [==============================] - 72s 146ms/step - loss: 6.6621 - val_loss: 6.2943\n","Epoch 4/50\n","492/492 [==============================] - 72s 146ms/step - loss: 6.1624 - val_loss: 5.7982\n","Epoch 5/50\n","492/492 [==============================] - 72s 146ms/step - loss: 5.6633 - val_loss: 5.3351\n","Epoch 6/50\n","492/492 [==============================] - 71s 145ms/step - loss: 5.1964 - val_loss: 4.9063\n","Epoch 7/50\n","492/492 [==============================] - 72s 146ms/step - loss: 4.7812 - val_loss: 4.5128\n","Epoch 8/50\n","492/492 [==============================] - 72s 145ms/step - loss: 4.3572 - val_loss: 4.1550\n","Epoch 9/50\n","492/492 [==============================] - 71s 144ms/step - loss: 4.0458 - val_loss: 3.8386\n","Epoch 10/50\n","492/492 [==============================] - 71s 145ms/step - loss: 3.7503 - val_loss: 3.5745\n","Epoch 11/50\n","492/492 [==============================] - 72s 146ms/step - loss: 3.4491 - val_loss: 3.3731\n","Epoch 12/50\n","492/492 [==============================] - 71s 144ms/step - loss: 3.2815 - val_loss: 3.2338\n","Epoch 13/50\n","492/492 [==============================] - 71s 145ms/step - loss: 3.1915 - val_loss: 3.1458\n","Epoch 14/50\n","492/492 [==============================] - 71s 145ms/step - loss: 3.0493 - val_loss: 3.0946\n","Epoch 15/50\n","492/492 [==============================] - 71s 145ms/step - loss: 2.9723 - val_loss: 3.0667\n","Epoch 16/50\n","492/492 [==============================] - 71s 145ms/step - loss: 3.0217 - val_loss: 3.0513\n","Epoch 17/50\n","492/492 [==============================] - 71s 144ms/step - loss: 2.9839 - val_loss: 3.0428\n","Epoch 18/50\n","492/492 [==============================] - 71s 144ms/step - loss: 2.9933 - val_loss: 3.0379\n","Epoch 19/50\n","492/492 [==============================] - 73s 148ms/step - loss: 2.9505 - val_loss: 3.0346\n","Epoch 20/50\n","492/492 [==============================] - 72s 145ms/step - loss: 3.0103 - val_loss: 3.0325\n","Epoch 21/50\n","492/492 [==============================] - 71s 145ms/step - loss: 2.9717 - val_loss: 3.0311\n","Epoch 22/50\n","492/492 [==============================] - 73s 149ms/step - loss: 2.9849 - val_loss: 3.0303\n","Epoch 23/50\n","492/492 [==============================] - 73s 149ms/step - loss: 2.9531 - val_loss: 3.0298\n","Epoch 24/50\n","492/492 [==============================] - 73s 148ms/step - loss: 2.9457 - val_loss: 3.0297\n","Epoch 25/50\n","492/492 [==============================] - 72s 146ms/step - loss: 3.0265 - val_loss: 3.0299\n","Epoch 26/50\n","492/492 [==============================] - 72s 146ms/step - loss: 3.0027 - val_loss: 3.0304\n","Epoch 27/50\n","492/492 [==============================] - 72s 146ms/step - loss: 2.9414 - val_loss: 3.0308\n","Epoch 28/50\n","492/492 [==============================] - 73s 149ms/step - loss: 3.0108 - val_loss: 3.0318\n","Epoch 29/50\n","492/492 [==============================] - 74s 150ms/step - loss: 2.8852 - val_loss: 3.0326\n","Epoch 30/50\n","492/492 [==============================] - 73s 148ms/step - loss: 2.9396 - val_loss: 3.0335\n","Epoch 31/50\n","492/492 [==============================] - 73s 148ms/step - loss: 2.9899 - val_loss: 3.0346\n","Epoch 32/50\n","492/492 [==============================] - 72s 146ms/step - loss: 2.9593 - val_loss: 3.0357\n","Epoch 33/50\n","492/492 [==============================] - 72s 146ms/step - loss: 2.9373 - val_loss: 3.0369\n","Epoch 34/50\n","492/492 [==============================] - 71s 145ms/step - loss: 2.9455 - val_loss: 3.0381\n","Epoch 35/50\n","492/492 [==============================] - 71s 144ms/step - loss: 2.9349 - val_loss: 3.0392\n","Epoch 36/50\n","492/492 [==============================] - 71s 145ms/step - loss: 2.9300 - val_loss: 3.0405\n","Epoch 37/50\n","492/492 [==============================] - 71s 145ms/step - loss: 2.9706 - val_loss: 3.0419\n","Epoch 38/50\n","492/492 [==============================] - 71s 145ms/step - loss: 2.9729 - val_loss: 3.0430\n","Epoch 39/50\n","492/492 [==============================] - 71s 145ms/step - loss: 2.9450 - val_loss: 3.0442\n","Epoch 40/50\n","492/492 [==============================] - 71s 145ms/step - loss: 2.9297 - val_loss: 3.0441\n","Epoch 41/50\n","492/492 [==============================] - 71s 145ms/step - loss: 2.9700 - val_loss: 3.0437\n","Epoch 42/50\n","492/492 [==============================] - 71s 145ms/step - loss: 2.9456 - val_loss: 3.0434\n","Epoch 43/50\n","492/492 [==============================] - 72s 146ms/step - loss: 2.9309 - val_loss: 3.0431\n","Epoch 44/50\n","492/492 [==============================] - 72s 146ms/step - loss: 2.9271 - val_loss: 3.0428\n","Epoch 45/50\n","492/492 [==============================] - 71s 144ms/step - loss: 2.9658 - val_loss: 3.0424\n","Epoch 46/50\n","492/492 [==============================] - 71s 145ms/step - loss: 3.0059 - val_loss: 3.0422\n","Epoch 47/50\n","492/492 [==============================] - 71s 145ms/step - loss: 2.9475 - val_loss: 3.0419\n","Epoch 48/50\n","492/492 [==============================] - 72s 146ms/step - loss: 2.9567 - val_loss: 3.0416\n","Epoch 49/50\n","492/492 [==============================] - 72s 146ms/step - loss: 2.9946 - val_loss: 3.0413\n","Epoch 50/50\n","492/492 [==============================] - 72s 146ms/step - loss: 2.9621 - val_loss: 3.0412\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f7c096b9250>"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"25p9my_Gxpmf"},"source":["Without using Mask Values."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":765},"id":"MZfBgBf9whtY","executionInfo":{"status":"error","timestamp":1617169083351,"user_tz":-330,"elapsed":509894,"user":{"displayName":"Nipun Agrawal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEnTNOPKYA066Ly1RwsNs4132AgrMH5fVw2BuEAw=s64","userId":"18285146514188802906"}},"outputId":"c25714cc-c521-4832-d09c-22a2f48f15fd"},"source":["EC.compile(optimizer=tf.keras.optimizers.Adam(),loss=tf.keras.losses.sparse_categorical_crossentropy)\n","EC.fit(x=[x,y],y=z,epochs=50,verbose=1,batch_size=4,validation_data=([encoder_test,decoder_inp_test],decoder_out_test))"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Epoch 1/50\n","492/492 [==============================] - 93s 104ms/step - loss: 3.6321 - val_loss: 2.3694\n","Epoch 2/50\n","492/492 [==============================] - 41s 83ms/step - loss: 2.2554 - val_loss: 2.3338\n","Epoch 3/50\n","492/492 [==============================] - 41s 84ms/step - loss: 2.2941 - val_loss: 2.3041\n","Epoch 4/50\n","492/492 [==============================] - 42s 85ms/step - loss: 2.1867 - val_loss: 2.2633\n","Epoch 5/50\n","492/492 [==============================] - 41s 84ms/step - loss: 2.1321 - val_loss: 2.2186\n","Epoch 6/50\n","492/492 [==============================] - 41s 84ms/step - loss: 2.0311 - val_loss: 2.1717\n","Epoch 7/50\n","492/492 [==============================] - 41s 84ms/step - loss: 1.9107 - val_loss: 2.1742\n","Epoch 8/50\n","492/492 [==============================] - 41s 84ms/step - loss: 1.8903 - val_loss: 2.1365\n","Epoch 9/50\n","492/492 [==============================] - 41s 84ms/step - loss: 1.8102 - val_loss: 2.1273\n","Epoch 10/50\n","492/492 [==============================] - 41s 84ms/step - loss: 1.7281 - val_loss: 2.1174\n","Epoch 11/50\n","492/492 [==============================] - 41s 84ms/step - loss: 1.6443 - val_loss: 2.0866\n","Epoch 12/50\n"," 42/492 [=>............................] - ETA: 37s - loss: 1.5815"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-d9836649fac4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mEC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_categorical_crossentropy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mEC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoder_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder_inp_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder_out_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":697},"id":"n7LlkDtmEOKK","executionInfo":{"status":"error","timestamp":1616840745962,"user_tz":-330,"elapsed":461612,"user":{"displayName":"Nipun Agrawal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEnTNOPKYA066Ly1RwsNs4132AgrMH5fVw2BuEAw=s64","userId":"18285146514188802906"}},"outputId":"8f03d05d-5e1c-4b3f-dca6-d5d008efd549"},"source":["EC.compile(optimizer=tf.keras.optimizers.Adam(0.01),loss=tf.keras.losses.sparse_categorical_crossentropy)\n","EC.fit(x=[x,y],y=z,epochs=260,verbose=1,batch_size=4,validation_data=([encoder_test,decoder_inp_test],decoder_out_test))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/260\n","492/492 [==============================] - 103s 113ms/step - loss: 3.0869 - val_loss: 2.5900\n","Epoch 2/260\n","492/492 [==============================] - 44s 90ms/step - loss: 2.4069 - val_loss: 2.4948\n","Epoch 3/260\n","492/492 [==============================] - 44s 90ms/step - loss: 2.4217 - val_loss: 2.4935\n","Epoch 4/260\n","492/492 [==============================] - 44s 90ms/step - loss: 2.3155 - val_loss: 2.4923\n","Epoch 5/260\n","492/492 [==============================] - 44s 90ms/step - loss: 2.1091 - val_loss: 2.5047\n","Epoch 6/260\n","492/492 [==============================] - 44s 89ms/step - loss: 2.0723 - val_loss: 2.4770\n","Epoch 7/260\n","492/492 [==============================] - 44s 90ms/step - loss: 2.0307 - val_loss: 2.4883\n","Epoch 8/260\n","492/492 [==============================] - 44s 89ms/step - loss: 1.8822 - val_loss: 2.5922\n","Epoch 9/260\n","492/492 [==============================] - 44s 89ms/step - loss: 1.9657 - val_loss: 2.6755\n","Epoch 10/260\n"," 52/492 [==>...........................] - ETA: 39s - loss: 2.0331"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-5a3134b86787>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mEC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_categorical_crossentropy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mEC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m260\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoder_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder_inp_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder_out_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ogYyMMfUlqNj","executionInfo":{"status":"ok","timestamp":1617095826316,"user_tz":-330,"elapsed":4688,"user":{"displayName":"Nipun Agrawal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEnTNOPKYA066Ly1RwsNs4132AgrMH5fVw2BuEAw=s64","userId":"18285146514188802906"}},"outputId":"29f3c873-bbed-443c-ab15-befa33600107"},"source":["!pip install tensorflow_addons"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting tensorflow_addons\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/e3/56d2fe76f0bb7c88ed9b2a6a557e25e83e252aec08f13de34369cd850a0b/tensorflow_addons-0.12.1-cp37-cp37m-manylinux2010_x86_64.whl (703kB)\n","\r\u001b[K     |▌                               | 10kB 25.9MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 18.4MB/s eta 0:00:01\r\u001b[K     |█▍                              | 30kB 15.0MB/s eta 0:00:01\r\u001b[K     |█▉                              | 40kB 13.8MB/s eta 0:00:01\r\u001b[K     |██▎                             | 51kB 9.4MB/s eta 0:00:01\r\u001b[K     |██▉                             | 61kB 8.8MB/s eta 0:00:01\r\u001b[K     |███▎                            | 71kB 9.9MB/s eta 0:00:01\r\u001b[K     |███▊                            | 81kB 11.0MB/s eta 0:00:01\r\u001b[K     |████▏                           | 92kB 9.9MB/s eta 0:00:01\r\u001b[K     |████▋                           | 102kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 112kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 122kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 133kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 143kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 153kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 163kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 174kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 184kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 194kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 204kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 215kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 225kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 235kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 245kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 256kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 266kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 276kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 286kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 296kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 307kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 317kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 327kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 337kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 348kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 358kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 368kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 378kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 389kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 399kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 409kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 419kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 430kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 440kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 450kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 460kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 471kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 481kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 491kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 501kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 512kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 522kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 532kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 542kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 552kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 563kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 573kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 583kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 593kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 604kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 614kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 624kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 634kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 645kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 655kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 665kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 675kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 686kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 696kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 706kB 9.0MB/s \n","\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n","Installing collected packages: tensorflow-addons\n","Successfully installed tensorflow-addons-0.12.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EO_tvWhilRov"},"source":["import tensorflow_addons as tfa\n","adamW= tfa.optimizers.AdamW(\n","    weight_decay=0.8,\n","    learning_rate = 0.001,\n","    beta_1 = 0.9,\n","    beta_2 = 0.999,\n","    epsilon = 1e-07,\n","    amsgrad = False,\n","    name= 'AdamW'   \n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mQ9dGSAFxGtF"},"source":["#Reference:https://github.com/bckenstler/CLR/blob/master/clr_callback.py\n","from tensorflow.keras.callbacks import *\n","from tensorflow.keras import backend as K\n","import numpy as np\n","\n","class CyclicLR(Callback):\n","    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n","    The method cycles the learning rate between two boundaries with\n","    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n","    The amplitude of the cycle can be scaled on a per-iteration or \n","    per-cycle basis.\n","    This class has three built-in policies, as put forth in the paper.\n","    \"triangular\":\n","        A basic triangular cycle w/ no amplitude scaling.\n","    \"triangular2\":\n","        A basic triangular cycle that scales initial amplitude by half each cycle.\n","    \"exp_range\":\n","        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n","        cycle iteration.\n","    For more detail, please see paper.\n","    \n","    # Example\n","        ```python\n","            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n","                                step_size=2000., mode='triangular')\n","            model.fit(X_train, Y_train, callbacks=[clr])\n","        ```\n","    \n","    Class also supports custom scaling functions:\n","        ```python\n","            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n","            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n","                                step_size=2000., scale_fn=clr_fn,\n","                                scale_mode='cycle')\n","            model.fit(X_train, Y_train, callbacks=[clr])\n","        ```    \n","    # Arguments\n","        base_lr: initial learning rate which is the\n","            lower boundary in the cycle.\n","        max_lr: upper boundary in the cycle. Functionally,\n","            it defines the cycle amplitude (max_lr - base_lr).\n","            The lr at any cycle is the sum of base_lr\n","            and some scaling of the amplitude; therefore \n","            max_lr may not actually be reached depending on\n","            scaling function.\n","        step_size: number of training iterations per\n","            half cycle. Authors suggest setting step_size\n","            2-8 x training iterations in epoch.\n","        mode: one of {triangular, triangular2, exp_range}.\n","            Default 'triangular'.\n","            Values correspond to policies detailed above.\n","            If scale_fn is not None, this argument is ignored.\n","        gamma: constant in 'exp_range' scaling function:\n","            gamma**(cycle iterations)\n","        scale_fn: Custom scaling policy defined by a single\n","            argument lambda function, where \n","            0 <= scale_fn(x) <= 1 for all x >= 0.\n","            mode paramater is ignored \n","        scale_mode: {'cycle', 'iterations'}.\n","            Defines whether scale_fn is evaluated on \n","            cycle number or cycle iterations (training\n","            iterations since start of cycle). Default is 'cycle'.\n","    \"\"\"\n","\n","    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n","                 gamma=1., scale_fn=None, scale_mode='cycle'):\n","        super(CyclicLR, self).__init__()\n","\n","        self.base_lr = base_lr\n","        self.max_lr = max_lr\n","        self.step_size = step_size\n","        self.mode = mode\n","        self.gamma = gamma\n","        if scale_fn == None:\n","            if self.mode == 'triangular':\n","                self.scale_fn = lambda x: 1.\n","                self.scale_mode = 'cycle'\n","            elif self.mode == 'triangular2':\n","                self.scale_fn = lambda x: 1/(2.**(x-1))\n","                self.scale_mode = 'cycle'\n","            elif self.mode == 'exp_range':\n","                self.scale_fn = lambda x: gamma**(x)\n","                self.scale_mode = 'iterations'\n","        else:\n","            self.scale_fn = scale_fn\n","            self.scale_mode = scale_mode\n","        self.clr_iterations = 0.\n","        self.trn_iterations = 0.\n","        self.history = {}\n","\n","        self._reset()\n","\n","    def _reset(self, new_base_lr=None, new_max_lr=None,\n","               new_step_size=None):\n","        \"\"\"Resets cycle iterations.\n","        Optional boundary/step size adjustment.\n","        \"\"\"\n","        if new_base_lr != None:\n","            self.base_lr = new_base_lr\n","        if new_max_lr != None:\n","            self.max_lr = new_max_lr\n","        if new_step_size != None:\n","            self.step_size = new_step_size\n","        self.clr_iterations = 0.\n","        \n","    def clr(self):\n","        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n","        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n","        if self.scale_mode == 'cycle':\n","            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n","        else:\n","            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n","        \n","    def on_train_begin(self, logs={}):\n","        logs = logs or {}\n","\n","        if self.clr_iterations == 0:\n","            K.set_value(self.model.optimizer.lr, self.base_lr)\n","        else:\n","            K.set_value(self.model.optimizer.lr, self.clr())        \n","            \n","    def on_batch_end(self, epoch, logs=None):\n","        \n","        logs = logs or {}\n","        self.trn_iterations += 1\n","        self.clr_iterations += 1\n","\n","        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n","        self.history.setdefault('iterations', []).append(self.trn_iterations)\n","\n","        for k, v in logs.items():\n","            self.history.setdefault(k, []).append(v)\n","        \n","        K.set_value(self.model.optimizer.lr, self.clr())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XBg0QRUTxPkE"},"source":["clr_triangular = CyclicLR(mode='triangular')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hYxQgVeol7yv","executionInfo":{"status":"ok","timestamp":1617097509445,"user_tz":-330,"elapsed":1658201,"user":{"displayName":"Nipun Agrawal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEnTNOPKYA066Ly1RwsNs4132AgrMH5fVw2BuEAw=s64","userId":"18285146514188802906"}},"outputId":"319c7cbc-f0d7-4c58-9b87-a08f704b660b"},"source":["EC.compile(optimizer=adamW,loss=tf.keras.losses.sparse_categorical_crossentropy)\n","EC.fit(x=[x,y],y=z,epochs=40,verbose=1,batch_size=4,validation_data=([encoder_test,decoder_inp_test],decoder_out_test),callbacks=[clr_triangular])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/40\n","492/492 [==============================] - 92s 102ms/step - loss: 7.9359 - val_loss: 8.0136\n","Epoch 2/40\n","492/492 [==============================] - 40s 81ms/step - loss: 8.0152 - val_loss: 8.0065\n","Epoch 3/40\n","492/492 [==============================] - 40s 81ms/step - loss: 6.9255 - val_loss: 8.0128\n","Epoch 4/40\n","492/492 [==============================] - 40s 82ms/step - loss: 8.0121 - val_loss: 8.0108\n","Epoch 5/40\n","492/492 [==============================] - 40s 81ms/step - loss: 8.0106 - val_loss: 8.0118\n","Epoch 6/40\n","492/492 [==============================] - 40s 81ms/step - loss: 8.0122 - val_loss: 8.0137\n","Epoch 7/40\n","492/492 [==============================] - 40s 82ms/step - loss: 8.0141 - val_loss: 8.0156\n","Epoch 8/40\n","492/492 [==============================] - 40s 82ms/step - loss: 8.0161 - val_loss: 8.0175\n","Epoch 9/40\n","492/492 [==============================] - 40s 82ms/step - loss: 8.0177 - val_loss: 8.0167\n","Epoch 10/40\n","492/492 [==============================] - 40s 81ms/step - loss: 8.0162 - val_loss: 8.0148\n","Epoch 11/40\n","492/492 [==============================] - 40s 81ms/step - loss: 8.0143 - val_loss: 8.0129\n","Epoch 12/40\n","492/492 [==============================] - 40s 81ms/step - loss: 8.0124 - val_loss: 8.0111\n","Epoch 13/40\n","492/492 [==============================] - 40s 81ms/step - loss: 8.0108 - val_loss: 8.0114\n","Epoch 14/40\n","492/492 [==============================] - 40s 82ms/step - loss: 8.0121 - val_loss: 8.0134\n","Epoch 15/40\n","492/492 [==============================] - 40s 82ms/step - loss: 8.0139 - val_loss: 8.0152\n","Epoch 16/40\n","492/492 [==============================] - 41s 82ms/step - loss: 8.0158 - val_loss: 8.0172\n","Epoch 17/40\n","492/492 [==============================] - 40s 82ms/step - loss: 8.0176 - val_loss: 8.0170\n","Epoch 18/40\n","492/492 [==============================] - 40s 82ms/step - loss: 8.0165 - val_loss: 8.0150\n","Epoch 19/40\n","492/492 [==============================] - 40s 81ms/step - loss: 8.0145 - val_loss: 8.0131\n","Epoch 20/40\n","492/492 [==============================] - 40s 81ms/step - loss: 8.0126 - val_loss: 8.0115\n","Epoch 21/40\n","492/492 [==============================] - 40s 81ms/step - loss: 8.0108 - val_loss: 8.0113\n","Epoch 22/40\n","492/492 [==============================] - 40s 82ms/step - loss: 8.0117 - val_loss: 8.0132\n","Epoch 23/40\n","492/492 [==============================] - 40s 81ms/step - loss: 8.0136 - val_loss: 8.0151\n","Epoch 24/40\n","492/492 [==============================] - 40s 81ms/step - loss: 8.0155 - val_loss: 8.0170\n","Epoch 25/40\n","492/492 [==============================] - 40s 81ms/step - loss: 8.0174 - val_loss: 8.0173\n","Epoch 26/40\n","492/492 [==============================] - 40s 81ms/step - loss: 8.0167 - val_loss: 8.0153\n","Epoch 27/40\n","492/492 [==============================] - 40s 81ms/step - loss: 8.0148 - val_loss: 8.0135\n","Epoch 28/40\n","492/492 [==============================] - 40s 81ms/step - loss: 8.0129 - val_loss: 8.0115\n","Epoch 29/40\n","492/492 [==============================] - 40s 81ms/step - loss: 8.0110 - val_loss: 8.0109\n","Epoch 30/40\n","492/492 [==============================] - 40s 81ms/step - loss: 8.0114 - val_loss: 8.0131\n","Epoch 31/40\n","492/492 [==============================] - 40s 82ms/step - loss: 8.0134 - val_loss: 8.0149\n","Epoch 32/40\n","492/492 [==============================] - 40s 81ms/step - loss: 8.0153 - val_loss: 8.0168\n","Epoch 33/40\n","492/492 [==============================] - 40s 81ms/step - loss: 8.0172 - val_loss: 8.0174\n","Epoch 34/40\n","492/492 [==============================] - 40s 81ms/step - loss: 8.0170 - val_loss: 8.0155\n","Epoch 35/40\n","492/492 [==============================] - 41s 83ms/step - loss: 8.0150 - val_loss: 8.0137\n","Epoch 36/40\n","492/492 [==============================] - 41s 83ms/step - loss: 8.0130 - val_loss: 8.0116\n","Epoch 37/40\n","492/492 [==============================] - 41s 82ms/step - loss: 8.0112 - val_loss: 8.0108\n","Epoch 38/40\n","492/492 [==============================] - 40s 82ms/step - loss: 8.0112 - val_loss: 8.0125\n","Epoch 39/40\n","492/492 [==============================] - 41s 83ms/step - loss: 8.0131 - val_loss: 8.0147\n","Epoch 40/40\n","492/492 [==============================] - 40s 82ms/step - loss: 8.0151 - val_loss: 8.0164\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f8b70d3bcd0>"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YvVJKbZY3tSd","executionInfo":{"status":"ok","timestamp":1617095774123,"user_tz":-330,"elapsed":8701829,"user":{"displayName":"Nipun Agrawal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEnTNOPKYA066Ly1RwsNs4132AgrMH5fVw2BuEAw=s64","userId":"18285146514188802906"}},"outputId":"3005940d-02cb-48a5-d9c8-5bf3efdc3f88"},"source":["EC.compile(optimizer=tf.keras.optimizers.SGD(0.01),loss=tf.keras.losses.sparse_categorical_crossentropy)\n","EC.fit(x=[x,y],y=z,epochs=260,verbose=1,batch_size=4,validation_data=([encoder_test,decoder_inp_test],decoder_out_test),callbacks=[clr_triangular])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/260\n","492/492 [==============================] - 115s 90ms/step - loss: 7.9496 - val_loss: 7.6320\n","Epoch 2/260\n","492/492 [==============================] - 34s 68ms/step - loss: 7.2261 - val_loss: 3.6574\n","Epoch 3/260\n","492/492 [==============================] - 33s 66ms/step - loss: 3.5300 - val_loss: 3.5623\n","Epoch 4/260\n","492/492 [==============================] - 33s 66ms/step - loss: 3.4411 - val_loss: 3.4381\n","Epoch 5/260\n","492/492 [==============================] - 33s 67ms/step - loss: 3.3367 - val_loss: 3.3311\n","Epoch 6/260\n","492/492 [==============================] - 33s 66ms/step - loss: 3.2589 - val_loss: 3.2775\n","Epoch 7/260\n","492/492 [==============================] - 33s 66ms/step - loss: 3.2374 - val_loss: 3.2409\n","Epoch 8/260\n","492/492 [==============================] - 33s 66ms/step - loss: 3.2486 - val_loss: 3.2196\n","Epoch 9/260\n","492/492 [==============================] - 33s 66ms/step - loss: 3.1802 - val_loss: 3.1991\n","Epoch 10/260\n","492/492 [==============================] - 33s 66ms/step - loss: 3.1130 - val_loss: 3.1687\n","Epoch 11/260\n","492/492 [==============================] - 33s 67ms/step - loss: 3.1085 - val_loss: 3.1127\n","Epoch 12/260\n","492/492 [==============================] - 33s 66ms/step - loss: 3.0248 - val_loss: 3.0024\n","Epoch 13/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.9090 - val_loss: 2.8885\n","Epoch 14/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.8065 - val_loss: 2.8219\n","Epoch 15/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.7421 - val_loss: 2.7845\n","Epoch 16/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.7140 - val_loss: 2.7630\n","Epoch 17/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.6820 - val_loss: 2.7527\n","Epoch 18/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.6918 - val_loss: 2.7254\n","Epoch 19/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.6817 - val_loss: 2.6973\n","Epoch 20/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.6249 - val_loss: 2.6844\n","Epoch 21/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.6390 - val_loss: 2.6346\n","Epoch 22/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.5623 - val_loss: 2.6029\n","Epoch 23/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.5188 - val_loss: 2.5830\n","Epoch 24/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.4928 - val_loss: 2.5765\n","Epoch 25/260\n","492/492 [==============================] - 32s 66ms/step - loss: 2.5482 - val_loss: 2.5733\n","Epoch 26/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.5504 - val_loss: 2.5608\n","Epoch 27/260\n","492/492 [==============================] - 32s 66ms/step - loss: 2.5570 - val_loss: 2.5561\n","Epoch 28/260\n","492/492 [==============================] - 32s 66ms/step - loss: 2.4579 - val_loss: 2.5454\n","Epoch 29/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.4924 - val_loss: 2.5193\n","Epoch 30/260\n","492/492 [==============================] - 33s 68ms/step - loss: 2.4331 - val_loss: 2.5109\n","Epoch 31/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.5027 - val_loss: 2.5063\n","Epoch 32/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.4138 - val_loss: 2.5108\n","Epoch 33/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.4049 - val_loss: 2.4903\n","Epoch 34/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.3973 - val_loss: 2.5134\n","Epoch 35/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.4327 - val_loss: 2.5034\n","Epoch 36/260\n","492/492 [==============================] - 33s 68ms/step - loss: 2.4804 - val_loss: 2.4749\n","Epoch 37/260\n","492/492 [==============================] - 33s 68ms/step - loss: 2.4415 - val_loss: 2.5281\n","Epoch 38/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.4269 - val_loss: 2.4589\n","Epoch 39/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.4223 - val_loss: 2.4538\n","Epoch 40/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.4478 - val_loss: 2.4467\n","Epoch 41/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.3899 - val_loss: 2.4449\n","Epoch 42/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.4222 - val_loss: 2.4629\n","Epoch 43/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.3560 - val_loss: 2.4411\n","Epoch 44/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.4036 - val_loss: 2.4342\n","Epoch 45/260\n","492/492 [==============================] - 32s 66ms/step - loss: 2.4083 - val_loss: 2.4389\n","Epoch 46/260\n","492/492 [==============================] - 32s 66ms/step - loss: 2.4354 - val_loss: 2.4240\n","Epoch 47/260\n","492/492 [==============================] - 32s 66ms/step - loss: 2.4501 - val_loss: 2.4182\n","Epoch 48/260\n","492/492 [==============================] - 32s 66ms/step - loss: 2.3733 - val_loss: 2.4179\n","Epoch 49/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.3599 - val_loss: 2.4172\n","Epoch 50/260\n","492/492 [==============================] - 32s 66ms/step - loss: 2.3940 - val_loss: 2.4161\n","Epoch 51/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.3869 - val_loss: 2.4109\n","Epoch 52/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.3602 - val_loss: 2.4044\n","Epoch 53/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.3668 - val_loss: 2.4093\n","Epoch 54/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.3532 - val_loss: 2.4137\n","Epoch 55/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.3841 - val_loss: 2.3981\n","Epoch 56/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.3791 - val_loss: 2.3936\n","Epoch 57/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.3372 - val_loss: 2.3971\n","Epoch 58/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.2996 - val_loss: 2.4038\n","Epoch 59/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.3035 - val_loss: 2.3938\n","Epoch 60/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.3679 - val_loss: 2.3876\n","Epoch 61/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.3876 - val_loss: 2.3909\n","Epoch 62/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.3483 - val_loss: 2.3890\n","Epoch 63/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.3573 - val_loss: 2.3819\n","Epoch 64/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.3064 - val_loss: 2.3767\n","Epoch 65/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.3563 - val_loss: 2.3758\n","Epoch 66/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.3151 - val_loss: 2.3799\n","Epoch 67/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.3427 - val_loss: 2.3742\n","Epoch 68/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.3746 - val_loss: 2.3688\n","Epoch 69/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.3241 - val_loss: 2.3733\n","Epoch 70/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.3380 - val_loss: 2.3656\n","Epoch 71/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.3433 - val_loss: 2.3667\n","Epoch 72/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.3454 - val_loss: 2.3682\n","Epoch 73/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.3155 - val_loss: 2.3622\n","Epoch 74/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.2945 - val_loss: 2.3639\n","Epoch 75/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.3424 - val_loss: 2.3628\n","Epoch 76/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.3810 - val_loss: 2.3624\n","Epoch 77/260\n","492/492 [==============================] - 32s 66ms/step - loss: 2.3258 - val_loss: 2.3635\n","Epoch 78/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.2791 - val_loss: 2.3568\n","Epoch 79/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.3219 - val_loss: 2.3567\n","Epoch 80/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.3599 - val_loss: 2.3501\n","Epoch 81/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.2967 - val_loss: 2.3504\n","Epoch 82/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.2571 - val_loss: 2.3512\n","Epoch 83/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.3056 - val_loss: 2.3505\n","Epoch 84/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.2818 - val_loss: 2.3474\n","Epoch 85/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.3472 - val_loss: 2.4076\n","Epoch 86/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.3115 - val_loss: 2.3498\n","Epoch 87/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.3345 - val_loss: 2.3455\n","Epoch 88/260\n","492/492 [==============================] - 32s 66ms/step - loss: 2.3851 - val_loss: 2.3422\n","Epoch 89/260\n","492/492 [==============================] - 32s 65ms/step - loss: 2.3133 - val_loss: 2.3467\n","Epoch 90/260\n","492/492 [==============================] - 32s 65ms/step - loss: 2.3041 - val_loss: 2.3413\n","Epoch 91/260\n","492/492 [==============================] - 32s 65ms/step - loss: 2.2899 - val_loss: 2.3493\n","Epoch 92/260\n","492/492 [==============================] - 32s 65ms/step - loss: 2.3333 - val_loss: 2.3416\n","Epoch 93/260\n","492/492 [==============================] - 32s 65ms/step - loss: 2.3532 - val_loss: 2.3391\n","Epoch 94/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.2762 - val_loss: 2.3390\n","Epoch 95/260\n","492/492 [==============================] - 32s 66ms/step - loss: 2.2824 - val_loss: 2.3389\n","Epoch 96/260\n","492/492 [==============================] - 32s 66ms/step - loss: 2.3674 - val_loss: 2.3431\n","Epoch 97/260\n","492/492 [==============================] - 32s 66ms/step - loss: 2.3518 - val_loss: 2.3334\n","Epoch 98/260\n","492/492 [==============================] - 32s 66ms/step - loss: 2.2585 - val_loss: 2.3340\n","Epoch 99/260\n","492/492 [==============================] - 32s 66ms/step - loss: 2.3115 - val_loss: 2.3374\n","Epoch 100/260\n","492/492 [==============================] - 32s 65ms/step - loss: 2.3129 - val_loss: 2.3345\n","Epoch 101/260\n","492/492 [==============================] - 32s 66ms/step - loss: 2.2710 - val_loss: 2.3312\n","Epoch 102/260\n","492/492 [==============================] - 32s 66ms/step - loss: 2.3133 - val_loss: 2.3429\n","Epoch 103/260\n","492/492 [==============================] - 32s 65ms/step - loss: 2.3189 - val_loss: 2.3306\n","Epoch 104/260\n","492/492 [==============================] - 32s 66ms/step - loss: 2.3580 - val_loss: 2.3295\n","Epoch 105/260\n","492/492 [==============================] - 32s 66ms/step - loss: 2.2955 - val_loss: 2.3312\n","Epoch 106/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.2839 - val_loss: 2.3275\n","Epoch 107/260\n","492/492 [==============================] - 32s 66ms/step - loss: 2.3114 - val_loss: 2.3391\n","Epoch 108/260\n","492/492 [==============================] - 32s 66ms/step - loss: 2.2618 - val_loss: 2.3353\n","Epoch 109/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.3184 - val_loss: 2.3261\n","Epoch 110/260\n","492/492 [==============================] - 32s 65ms/step - loss: 2.3076 - val_loss: 2.3308\n","Epoch 111/260\n","492/492 [==============================] - 32s 66ms/step - loss: 2.2983 - val_loss: 2.3270\n","Epoch 112/260\n","492/492 [==============================] - 32s 66ms/step - loss: 2.2319 - val_loss: 2.3252\n","Epoch 113/260\n","492/492 [==============================] - 32s 66ms/step - loss: 2.2468 - val_loss: 2.3231\n","Epoch 114/260\n","492/492 [==============================] - 32s 66ms/step - loss: 2.2985 - val_loss: 2.3237\n","Epoch 115/260\n","492/492 [==============================] - 32s 66ms/step - loss: 2.2759 - val_loss: 2.3218\n","Epoch 116/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.3329 - val_loss: 2.3247\n","Epoch 117/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.3022 - val_loss: 2.3210\n","Epoch 118/260\n","492/492 [==============================] - 32s 66ms/step - loss: 2.3176 - val_loss: 2.3409\n","Epoch 119/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.3268 - val_loss: 2.3220\n","Epoch 120/260\n","492/492 [==============================] - 32s 66ms/step - loss: 2.2662 - val_loss: 2.3195\n","Epoch 121/260\n","492/492 [==============================] - 32s 66ms/step - loss: 2.3201 - val_loss: 2.3163\n","Epoch 122/260\n","492/492 [==============================] - 32s 66ms/step - loss: 2.2517 - val_loss: 2.3181\n","Epoch 123/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.3156 - val_loss: 2.3188\n","Epoch 124/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.3048 - val_loss: 2.3170\n","Epoch 125/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.2781 - val_loss: 2.3204\n","Epoch 126/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.2693 - val_loss: 2.3204\n","Epoch 127/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.2272 - val_loss: 2.3574\n","Epoch 128/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.3507 - val_loss: 2.3143\n","Epoch 129/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.2666 - val_loss: 2.3128\n","Epoch 130/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.2897 - val_loss: 2.3149\n","Epoch 131/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.2621 - val_loss: 2.3171\n","Epoch 132/260\n","492/492 [==============================] - 32s 65ms/step - loss: 2.2857 - val_loss: 2.3188\n","Epoch 133/260\n","492/492 [==============================] - 32s 65ms/step - loss: 2.2723 - val_loss: 2.3193\n","Epoch 134/260\n","492/492 [==============================] - 32s 65ms/step - loss: 2.3181 - val_loss: 2.3198\n","Epoch 135/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.2370 - val_loss: 2.3521\n","Epoch 136/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.2602 - val_loss: 2.3122\n","Epoch 137/260\n","492/492 [==============================] - 32s 65ms/step - loss: 2.2791 - val_loss: 2.3148\n","Epoch 138/260\n","492/492 [==============================] - 32s 66ms/step - loss: 2.2815 - val_loss: 2.3126\n","Epoch 139/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.2693 - val_loss: 2.3139\n","Epoch 140/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.2609 - val_loss: 2.3149\n","Epoch 141/260\n","492/492 [==============================] - 32s 66ms/step - loss: 2.2818 - val_loss: 2.3515\n","Epoch 142/260\n","492/492 [==============================] - 32s 66ms/step - loss: 2.2033 - val_loss: 2.3181\n","Epoch 143/260\n","492/492 [==============================] - 32s 66ms/step - loss: 2.3108 - val_loss: 2.3142\n","Epoch 144/260\n","492/492 [==============================] - 33s 66ms/step - loss: 2.3088 - val_loss: 2.3135\n","Epoch 145/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.2568 - val_loss: 2.3061\n","Epoch 146/260\n","492/492 [==============================] - 32s 66ms/step - loss: 2.2726 - val_loss: 2.3069\n","Epoch 147/260\n","492/492 [==============================] - 32s 65ms/step - loss: 2.3350 - val_loss: 2.3080\n","Epoch 148/260\n","492/492 [==============================] - 32s 66ms/step - loss: 2.2658 - val_loss: 2.3084\n","Epoch 149/260\n","492/492 [==============================] - 32s 66ms/step - loss: 2.2520 - val_loss: 2.3250\n","Epoch 150/260\n","492/492 [==============================] - 32s 65ms/step - loss: 2.2941 - val_loss: 2.3100\n","Epoch 151/260\n","492/492 [==============================] - 32s 66ms/step - loss: 2.2488 - val_loss: 2.3046\n","Epoch 152/260\n","492/492 [==============================] - 33s 68ms/step - loss: 2.2949 - val_loss: 2.3070\n","Epoch 153/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2801 - val_loss: 2.3076\n","Epoch 154/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2633 - val_loss: 2.3027\n","Epoch 155/260\n","492/492 [==============================] - 34s 68ms/step - loss: 2.3040 - val_loss: 2.3029\n","Epoch 156/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.3075 - val_loss: 2.3055\n","Epoch 157/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.3131 - val_loss: 2.3109\n","Epoch 158/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2845 - val_loss: 2.3120\n","Epoch 159/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2744 - val_loss: 2.3073\n","Epoch 160/260\n","492/492 [==============================] - 34s 68ms/step - loss: 2.3245 - val_loss: 2.3150\n","Epoch 161/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2892 - val_loss: 2.3025\n","Epoch 162/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2367 - val_loss: 2.3024\n","Epoch 163/260\n","492/492 [==============================] - 34s 70ms/step - loss: 2.2472 - val_loss: 2.3069\n","Epoch 164/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2989 - val_loss: 2.3053\n","Epoch 165/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.1946 - val_loss: 2.3040\n","Epoch 166/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2788 - val_loss: 2.3050\n","Epoch 167/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2878 - val_loss: 2.3029\n","Epoch 168/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2815 - val_loss: 2.3025\n","Epoch 169/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.3061 - val_loss: 2.3118\n","Epoch 170/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2801 - val_loss: 2.3080\n","Epoch 171/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.3071 - val_loss: 2.3016\n","Epoch 172/260\n","492/492 [==============================] - 34s 70ms/step - loss: 2.3187 - val_loss: 2.3059\n","Epoch 173/260\n","492/492 [==============================] - 34s 70ms/step - loss: 2.2333 - val_loss: 2.3196\n","Epoch 174/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2395 - val_loss: 2.3164\n","Epoch 175/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2649 - val_loss: 2.2977\n","Epoch 176/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2870 - val_loss: 2.3016\n","Epoch 177/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.3088 - val_loss: 2.2990\n","Epoch 178/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2644 - val_loss: 2.3129\n","Epoch 179/260\n","492/492 [==============================] - 34s 70ms/step - loss: 2.2736 - val_loss: 2.3001\n","Epoch 180/260\n","492/492 [==============================] - 34s 70ms/step - loss: 2.2673 - val_loss: 2.2967\n","Epoch 181/260\n","492/492 [==============================] - 35s 70ms/step - loss: 2.2873 - val_loss: 2.3041\n","Epoch 182/260\n","492/492 [==============================] - 34s 70ms/step - loss: 2.2808 - val_loss: 2.3022\n","Epoch 183/260\n","492/492 [==============================] - 34s 70ms/step - loss: 2.2477 - val_loss: 2.2970\n","Epoch 184/260\n","492/492 [==============================] - 34s 70ms/step - loss: 2.2701 - val_loss: 2.3086\n","Epoch 185/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2718 - val_loss: 2.3054\n","Epoch 186/260\n","492/492 [==============================] - 34s 70ms/step - loss: 2.2286 - val_loss: 2.3058\n","Epoch 187/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2472 - val_loss: 2.3029\n","Epoch 188/260\n","492/492 [==============================] - 34s 70ms/step - loss: 2.2747 - val_loss: 2.3049\n","Epoch 189/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2889 - val_loss: 2.2969\n","Epoch 190/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2694 - val_loss: 2.3030\n","Epoch 191/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2271 - val_loss: 2.2973\n","Epoch 192/260\n","492/492 [==============================] - 34s 68ms/step - loss: 2.2336 - val_loss: 2.3142\n","Epoch 193/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2257 - val_loss: 2.3153\n","Epoch 194/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2885 - val_loss: 2.2972\n","Epoch 195/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2793 - val_loss: 2.2971\n","Epoch 196/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.3007 - val_loss: 2.2959\n","Epoch 197/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2729 - val_loss: 2.2958\n","Epoch 198/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2190 - val_loss: 2.3287\n","Epoch 199/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.3430 - val_loss: 2.3003\n","Epoch 200/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2915 - val_loss: 2.2969\n","Epoch 201/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2782 - val_loss: 2.2986\n","Epoch 202/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2317 - val_loss: 2.3101\n","Epoch 203/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2836 - val_loss: 2.2982\n","Epoch 204/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2456 - val_loss: 2.2970\n","Epoch 205/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2770 - val_loss: 2.2990\n","Epoch 206/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2871 - val_loss: 2.3031\n","Epoch 207/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2859 - val_loss: 2.2965\n","Epoch 208/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2517 - val_loss: 2.3015\n","Epoch 209/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2621 - val_loss: 2.3147\n","Epoch 210/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2795 - val_loss: 2.2993\n","Epoch 211/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2488 - val_loss: 2.2951\n","Epoch 212/260\n","492/492 [==============================] - 33s 68ms/step - loss: 2.3477 - val_loss: 2.2985\n","Epoch 213/260\n","492/492 [==============================] - 34s 68ms/step - loss: 2.2157 - val_loss: 2.3009\n","Epoch 214/260\n","492/492 [==============================] - 33s 68ms/step - loss: 2.2813 - val_loss: 2.2977\n","Epoch 215/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2505 - val_loss: 2.2995\n","Epoch 216/260\n","492/492 [==============================] - 34s 68ms/step - loss: 2.2259 - val_loss: 2.2960\n","Epoch 217/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.3174 - val_loss: 2.2959\n","Epoch 218/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2516 - val_loss: 2.3008\n","Epoch 219/260\n","492/492 [==============================] - 34s 68ms/step - loss: 2.2979 - val_loss: 2.2919\n","Epoch 220/260\n","492/492 [==============================] - 34s 68ms/step - loss: 2.2455 - val_loss: 2.2945\n","Epoch 221/260\n","492/492 [==============================] - 34s 68ms/step - loss: 2.2830 - val_loss: 2.2956\n","Epoch 222/260\n","492/492 [==============================] - 34s 68ms/step - loss: 2.2610 - val_loss: 2.2975\n","Epoch 223/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2203 - val_loss: 2.3142\n","Epoch 224/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2567 - val_loss: 2.3079\n","Epoch 225/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2387 - val_loss: 2.3126\n","Epoch 226/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2793 - val_loss: 2.2912\n","Epoch 227/260\n","492/492 [==============================] - 34s 70ms/step - loss: 2.2200 - val_loss: 2.3023\n","Epoch 228/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2386 - val_loss: 2.2989\n","Epoch 229/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2210 - val_loss: 2.2966\n","Epoch 230/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2209 - val_loss: 2.3229\n","Epoch 231/260\n","492/492 [==============================] - 34s 68ms/step - loss: 2.2322 - val_loss: 2.3331\n","Epoch 232/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.2351 - val_loss: 2.3085\n","Epoch 233/260\n","492/492 [==============================] - 33s 68ms/step - loss: 2.2863 - val_loss: 2.3115\n","Epoch 234/260\n","492/492 [==============================] - 33s 68ms/step - loss: 2.2787 - val_loss: 2.2963\n","Epoch 235/260\n","492/492 [==============================] - 35s 71ms/step - loss: 2.2398 - val_loss: 2.2952\n","Epoch 236/260\n","492/492 [==============================] - 35s 71ms/step - loss: 2.2149 - val_loss: 2.2934\n","Epoch 237/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2413 - val_loss: 2.2970\n","Epoch 238/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.3077 - val_loss: 2.3117\n","Epoch 239/260\n","492/492 [==============================] - 34s 68ms/step - loss: 2.2732 - val_loss: 2.2978\n","Epoch 240/260\n","492/492 [==============================] - 34s 68ms/step - loss: 2.2753 - val_loss: 2.3214\n","Epoch 241/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2108 - val_loss: 2.2942\n","Epoch 242/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.2264 - val_loss: 2.3268\n","Epoch 243/260\n","492/492 [==============================] - 33s 68ms/step - loss: 2.2653 - val_loss: 2.2951\n","Epoch 244/260\n","492/492 [==============================] - 35s 71ms/step - loss: 2.2776 - val_loss: 2.3051\n","Epoch 245/260\n","492/492 [==============================] - 34s 70ms/step - loss: 2.1869 - val_loss: 2.2922\n","Epoch 246/260\n","492/492 [==============================] - 35s 70ms/step - loss: 2.2760 - val_loss: 2.2963\n","Epoch 247/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2458 - val_loss: 2.3150\n","Epoch 248/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2444 - val_loss: 2.2894\n","Epoch 249/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2632 - val_loss: 2.2957\n","Epoch 250/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2936 - val_loss: 2.3017\n","Epoch 251/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2407 - val_loss: 2.2960\n","Epoch 252/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2058 - val_loss: 2.2968\n","Epoch 253/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2350 - val_loss: 2.2976\n","Epoch 254/260\n","492/492 [==============================] - 34s 68ms/step - loss: 2.2519 - val_loss: 2.2982\n","Epoch 255/260\n","492/492 [==============================] - 34s 69ms/step - loss: 2.2530 - val_loss: 2.3331\n","Epoch 256/260\n","492/492 [==============================] - 33s 68ms/step - loss: 2.2425 - val_loss: 2.3094\n","Epoch 257/260\n","492/492 [==============================] - 33s 68ms/step - loss: 2.1709 - val_loss: 2.2980\n","Epoch 258/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.2060 - val_loss: 2.2945\n","Epoch 259/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.2396 - val_loss: 2.2965\n","Epoch 260/260\n","492/492 [==============================] - 33s 67ms/step - loss: 2.2565 - val_loss: 2.3051\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f8be917dc90>"]},"metadata":{"tags":[]},"execution_count":18}]}]}